{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8f6464a-0c8b-4120-a72d-feef17af34e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f20b6c9-c0fb-4d47-81eb-8cf6b1315f79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bcda98-affa-4d15-8fb3-58b0c75cc23c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!module load conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2850d3-4da6-4395-bad3-55cc1a0ee15e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!conda activate /pscratch/sd/m/mansisak/memorization/env/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07607395-fd1e-43de-8a85-a6c3ee16e9a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls /pscratch/sd/m/mansisak/memorization/model_ckpts/EleutherAI_edit/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ea4b9e-c194-4272-9c27-f9db487c97cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf91b5b6-b20b-4a16-9982-5d6fd50017f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import copy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab27cb6-f7d1-4d7c-bc0b-06aa85db5a94",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load in Results + Visualize Tables of unlearning @ different time points/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caeb3b8-a15b-4e0a-9137-b0d7960a12e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_path = \"/pscratch/sd/m/mansisak/memorization/model_ckpts/EleutherAI_edit/\"\n",
    "\n",
    "file_name = \"localization_results_72000.csv\"\n",
    "\n",
    "df = pd.read_csv(f'{result_path}{file_name}')\n",
    "\n",
    "base_stats = df.loc[0].copy()\n",
    "df = df.drop_duplicates()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280dccd8-de20-4168-a224-cad684559897",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[df['unlearn_set_name'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33e369d-c092-4640-9cdc-c70f2e1bd504",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import statistics\n",
    "import math\n",
    "def return_ideal_method_for_lang(base_early_stats, weight_early, backdoor=False):\n",
    "  #desired columns\n",
    "  cols = [\"perc\", \"perp\",'localization_method','ratio']\n",
    "\n",
    "  #Convert base stats to a df\n",
    "  base_frame = base_early_stats.to_frame().T[cols]\n",
    "\n",
    "  #Generate Percent Differences\n",
    "  weight_early[\"perc_diff\"] =  (weight_early['perc'] - base_early_stats['perc']) /  base_early_stats['perc']\n",
    "\n",
    "\n",
    "  weight_early[\"perp_diff\"] = (weight_early['perp'] - base_early_stats['perp']) /  base_early_stats['perp']\n",
    "\n",
    "  weight_early[\"score\"] = 0 # zero initialize -- we will populate it below\n",
    "\n",
    "  #Compute Scores\n",
    "  for index, row in weight_early.iterrows():\n",
    "\n",
    "\n",
    "    if row['unlearn_set_name'] == \"mem\":\n",
    "        if row['perc_diff'] == 0:\n",
    "            weight_early.loc[index, \"score\"] += 100 # this is how we penalize zero perc drop in mem\n",
    "        weight_early.loc[index, \"score\"] += (row['perc_diff'])\n",
    "\n",
    "\n",
    "\n",
    "        weight_early.loc[index, \"score\"] += statistics.mean([row['perp_diff']])\n",
    "\n",
    "  df = pd.DataFrame(columns=cols)\n",
    "  #append base stats to df\n",
    "  base_frame['localization_method'] = \"BASE_STATS\"\n",
    "  df = pd.concat([df, base_frame])\n",
    "\n",
    "  #Print out ideal method for each dist based on the min score\n",
    "  for unlearn_set in ['greedy', 'durable', 'durable_agg', 'random', 'random_greedy', \"hc\", \"slim\", 'act', ]:\n",
    "    #print(unlearn_set)\n",
    "    subset = weight_early.loc[weight_early['localization_method'] == unlearn_set]\n",
    "    if subset.shape[0] == 0:\n",
    "      #print(\"no results yet for: \", unlearn_set)\n",
    "      continue\n",
    "    sub_frame = subset[subset.score == subset.score.min()].drop_duplicates()\n",
    "    sub_frame = sub_frame[cols]\n",
    "    #print(sub_frame)\n",
    "    df = pd.concat([df, sub_frame])\n",
    "\n",
    "  df[['ratio',]] *= 100\n",
    "\n",
    "  return df.drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f9b352-76fe-4634-8e76-aac36e92de21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "result_path = \"/pscratch/sd/m/mansisak/memorization/model_ckpts/EleutherAI_edit/\"\n",
    "dfs = []\n",
    "x_idx = 0\n",
    "for model_name in ['EleutherAI/pythia-6.9b-deduped', 'EleutherAI/pythia-2.8b-deduped']:\n",
    "    y_idx = 0\n",
    "    for step in [36000, 72000, 108000, 143000]:\n",
    "        print(\"STEP: \", step, \" model_name:  \", model_name)\n",
    "        file_name = f\"localization_results_{step}.csv\"\n",
    "\n",
    "        df = pd.read_csv(f'{result_path}{file_name}')\n",
    "        for col in df.columns:\n",
    "            if col in [\"perc\"]:\n",
    "                df[col] = df[col].apply(lambda x: round(x, 2))\n",
    "                df[col] = df[col].apply(lambda x: int(x*100))\n",
    "            if col in [\"perp\"]:\n",
    "                df[col] = df[col].apply(lambda x: int(x))\n",
    "            if col in ['ratio']:\n",
    "                df[col] = df[col].apply(lambda x: round(x, 6))\n",
    "        df = df[df['model_name'] == model_name]\n",
    "        #base_stats = df.loc[0].copy()\n",
    "        base_stats = df[df['unlearn_set_name'].isna()].iloc[0]\n",
    "        df = df.drop_duplicates()\n",
    "        #df = return_ideal_method_for_lang(base_stats, df, backdoor=False)\n",
    "        #print(df)\n",
    "        dfs.append(copy.deepcopy(df))\n",
    "        sns.scatterplot(x=\"perp\",\n",
    "                y=\"perc\",\n",
    "                data=df,\n",
    "                hue=\"localization_method\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea00510d-3303-44f4-82ce-f71ac21df166",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots( 2, 4, sharex='col', sharey='row')\n",
    "fig.suptitle('Unlearning Methods Comparison')\n",
    "fig.supylabel(\"Percent Memorized (%)\")\n",
    "\n",
    "\n",
    "y_idx = 0\n",
    "x_idx = 0\n",
    "\n",
    "result_path = \"/pscratch/sd/m/mansisak/memorization/model_ckpts/EleutherAI_edit/\"\n",
    "dfs = []\n",
    "x_idx = 0\n",
    "for model_name in ['EleutherAI/pythia-6.9b-deduped', 'EleutherAI/pythia-2.8b-deduped']:\n",
    "    y_idx = 0\n",
    "    for step in [36000, 72000, 108000, 143000]:\n",
    "        print(\"STEP: \", step, \" model_name:  \", model_name)\n",
    "        file_name = f\"localization_results_{step}.csv\"\n",
    "\n",
    "        df = pd.read_csv(f'{result_path}{file_name}')\n",
    "        for col in df.columns:\n",
    "            if col in [\"perc\"]:\n",
    "                df[col] = df[col].apply(lambda x: round(x, 2))\n",
    "                df[col] = df[col].apply(lambda x: int(x*100))\n",
    "            if col in [\"perp\"]:\n",
    "                df[col] = df[col].apply(lambda x: int(x))\n",
    "            if col in ['ratio']:\n",
    "                df[col] = df[col].apply(lambda x: round(x, 6))\n",
    "        df = df[df['model_name'] == model_name]\n",
    "        #base_stats = df.loc[0].copy()\n",
    "        base_stats = df[df['unlearn_set_name'].isna()].iloc[0]\n",
    "        df = df.drop_duplicates()\n",
    "        #df = return_ideal_method_for_lang(base_stats, df, backdoor=False)\n",
    "        #print(df)\n",
    "        dfs.append(copy.deepcopy(df))\n",
    "        ax = sns.scatterplot(x=\"perp\",\n",
    "                y=\"perc\",\n",
    "                data=df,\n",
    "                hue=\"localization_method\", ax=axs[x_idx, y_idx])\n",
    "        ax.set_xlabel(\"Perplexity\")\n",
    "        ax.set_ylabel(\"Percent Memorized (%)\")\n",
    "        ax.get_legend().remove()\n",
    "\n",
    "        print(y_idx, x_idx)\n",
    "        y_idx += 1\n",
    "    x_idx += 1\n",
    "\n",
    "rows = [\"Pythia 6.9B\", \"Pythia 2.8B\"]\n",
    "for ax, row in zip(axs[:,0], rows):\n",
    "    ax.set_ylabel(row)\n",
    "\n",
    "cols = [36000, 72000, 108000, 143000]\n",
    "for ax, col in zip(axs[0], cols):\n",
    "    ax.set_title(f'{col} steps')\n",
    "        #plt.show()\n",
    "\n",
    "#fig.legend(lines, labels, loc = (0.5, 0), ncol=5)\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "#fig.legend(handles, labels, loc='lower center')\n",
    "fig.legend(handles = handles , labels=labels,loc='upper center', \n",
    "             bbox_to_anchor=(0.5, -0.0),fancybox=False, shadow=False, ncol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692cbd5d-c96f-4cb0-88f7-956bf24c39fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_path = \"/pscratch/sd/m/mansisak/memorization/model_ckpts/EleutherAI_edit/\"\n",
    "dfs = []\n",
    "for model_name in ['EleutherAI/pythia-6.9b-deduped', 'EleutherAI/pythia-2.8b-deduped']:\n",
    "    for step in [36000, 72000, 108000, 143000]:\n",
    "        print(\"STEP: \", step, \" model_name:  \", model_name)\n",
    "        file_name = f\"localization_results_{step}.csv\"\n",
    "\n",
    "        df = pd.read_csv(f'{result_path}{file_name}')\n",
    "        for col in df.columns:\n",
    "            if col in [\"perc\"]:\n",
    "                df[col] = df[col].apply(lambda x: round(x, 2))\n",
    "                df[col] = df[col].apply(lambda x: int(x*100))\n",
    "            if col in [\"perp\"]:\n",
    "                df[col] = df[col].apply(lambda x: int(x))\n",
    "            if col in ['ratio']:\n",
    "                df[col] = df[col].apply(lambda x: round(x, 6))\n",
    "        df = df[df['model_name'] == model_name]\n",
    "        #base_stats = df.loc[0].copy()\n",
    "        base_stats = df[df['unlearn_set_name'].isna()].iloc[0]\n",
    "        df = df.drop_duplicates()\n",
    "        dfs.append(copy.deepcopy(df))\n",
    "\n",
    "        df = return_ideal_method_for_lang(base_stats, df, backdoor=False)\n",
    "        print(df)\n",
    "        print(df.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300f5b76-2d63-48f4-9d7e-f4cd214e0cbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "super_df = pd.concat(dfs, axis=0)\n",
    "sns.scatterplot(x=\"perp\",\n",
    "                y=\"perc\",\n",
    "                data=super_df,\n",
    "                hue=\"localization_method\")\n",
    "plt.title(\"Unlearning method comparision accorss models + timepoints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e89aaf-5236-4531-a9c3-8e91f740cfaa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Unlearn Accross Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bc3584-74e6-4731-a7eb-ca9495c2e547",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_before = []\n",
    "seq_after = []\n",
    "ckpt_dir = \"/pscratch/sd/m/mansisak/memorization/model_ckpts/\"\n",
    "steps = [108000, 143000]\n",
    "for step in steps:\n",
    "\n",
    "    #load original mem set\n",
    "    mem_seq_original = torch.load(f'{ckpt_dir}{step}/EleutherAI_edit/mem_seq_pythia-2.8b-deduped',map_location=torch.device('cpu'))\n",
    "    seq_before.append(mem_seq_original)\n",
    "\n",
    "    #load mem set after unlearning\n",
    "    #mem_seq_after_edit = torch.load(f'{ckpt_dir}{step}/EleutherAI_edit/act/mem/0.01/mem_seq_pythia-2.8b-deduped')\n",
    "    #mem_seq_after_edit = torch.load(f'{ckpt_dir}{step}/EleutherAI_edit/random_greedy/mem/0.01/1/0.1/0.9/0.0005/mem_seq_pythia-2.8b-deduped')\n",
    "    mem_seq_after_edit = torch.load(f'{ckpt_dir}{step}/EleutherAI_edit/hc/mem/0.01/1/1000/0.1/0.1/mem_seq_pythia-2.8b-deduped',map_location=torch.device('cpu'))\n",
    "    seq_after.append(mem_seq_after_edit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b22f4f-7fd7-4108-a534-294a51578060",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_common_mem_seq_at_all_time(seq_before):\n",
    "    #bc we unlearn at different time points, we want to find a common subset of points that is memorized at each training set\n",
    "    \n",
    "    base_mem_seq = seq_before[0] # the earliest timepiont has fewest mem seqence\n",
    "    common_seqs = []\n",
    "    for row in base_mem_seq: #iterate thru all rows of base mem seq\n",
    "        common = False\n",
    "        for seq in seq_before: #see if later time points contain it\n",
    "            if row in seq:\n",
    "                common=True\n",
    "                \n",
    "        if common:\n",
    "            common_seqs.append(copy.deepcopy(row))\n",
    "    \n",
    "    return torch.stack(common_seqs, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d4fb6f-712a-450b-9f32-539ffa217221",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "common_seqs = get_common_mem_seq_at_all_time(seq_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bebf69-4f5f-4295-b966-1c790ef9cf61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "common_seqs.shape #there are 80 common seqences memorized at each time point (we will compare unlearning methods on these 80 sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7722b34d-6401-449a-90ed-aec0a8153d1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_overlap(seq_after, common_seqs, steps=steps):\n",
    "    #Now we want to compare unlearning at multiple timepoints!\n",
    "    not_unlearn_set_over_time = []\n",
    "    for seq in seq_after:\n",
    "        not_unlearn_set = []\n",
    "        for row in seq:\n",
    "            if row in common_seqs:\n",
    "                #print(\"row not unlearned\")\n",
    "                not_unlearn_set.append(copy.deepcopy(row))\n",
    "        print(torch.stack(not_unlearn_set, dim=0).shape)\n",
    "        not_unlearned_set = torch.stack(not_unlearn_set, dim=0)\n",
    "        not_unlearn_set_over_time.append(copy.deepcopy(not_unlearned_set))\n",
    "        \n",
    "    for i in range(len(steps)):\n",
    "        step = steps[i]\n",
    "        print(f\"Perc unlearn @ step {step}: \", 100 * (common_seqs.shape[0] - not_unlearn_set_over_time[i].shape[0])/ common_seqs.shape[0], \"%\")\n",
    "    \n",
    "    return not_unlearn_set_over_time # we wnat to know how many of these sequenes were unlearned overtime\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33c711d-30d4-4dc1-ad39-17e0c388b67c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "not_unlearn_set_over_time = check_overlap(seq_after, common_seqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9b606e-ed78-4497-a1d3-9c87f09c6a9b",
   "metadata": {},
   "source": [
    "Interestingly in the above setting, it appears the same unlearning method (act), unlearns better at later timesteps, rather than earlier timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628213af-2184-4b3f-92bc-0bf4d6e58be0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# interpret results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2384df24-7a97-4bbe-b260-efc649a1c8df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "not_unlearn_set_over_time[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec542f4-38b7-41a5-8c9d-a5dae068d325",
   "metadata": {},
   "source": [
    "# Git commit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75447702-0dec-4d8c-8ab0-d33f4ead394d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!git add /pscratch/sd/m/mansisak/memorization/figs/pythia_unlearning_results.ipynb\n",
    "!git commit -m \"updated unlearning visualization/analysis\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NERSC Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
