Thu Aug  8 11:16:01 2024       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA A100-SXM...  On   | 00000000:03:00.0 Off |                    0 |
| N/A   28C    P0    60W / 500W |      0MiB / 81920MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA A100-SXM...  On   | 00000000:41:00.0 Off |                    0 |
| N/A   27C    P0    60W / 500W |      0MiB / 81920MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   2  NVIDIA A100-SXM...  On   | 00000000:82:00.0 Off |                    0 |
| N/A   28C    P0    63W / 500W |      0MiB / 81920MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
|   3  NVIDIA A100-SXM...  On   | 00000000:C1:00.0 Off |                    0 |
| N/A   27C    P0    61W / 500W |      0MiB / 81920MiB |      0%      Default |
|                               |                      |             Disabled |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
/pscratch/sd/m/mansisak/memorization/env/bin/python
nid008209
/pscratch/sd/m/mansisak/memorization/src/localize
Found cores : 128
Found nodes : 4
0: CRAY_CUDATOOLKIT_POST_LINK_OPTS=-L/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2/lib64 -L/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2/nvvm/lib64 -L/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2/extras/Debugger/lib64 -L/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2/extras/CUPTI/lib64 -L/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/math_libs/12.2/lib64 -Wl,--as-needed,-lcupti,-lcudart,--no-as-needed -lcuda
0: CUDA_HOME=/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2
0: CRAY_CUDATOOLKIT_DIR=/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2
0: NVHPC_CUDA_HOME=/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2
0: CRAY_CUDATOOLKIT_INCLUDE_OPTS=-I/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2/include -I/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2/nvvm/include -I/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2/extras/Debugger/include -I/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2/extras/CUPTI/include -I/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/math_libs/12.2/include
0: CRAY_CUDATOOLKIT_PREFIX=/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2
0: LMOD_FAMILY_CUDATOOLKIT_VERSION=12.2
0: LMOD_FAMILY_CUDATOOLKIT=cudatoolkit
0: CUDA_VISIBLE_DEVICES=0
0: CUDATOOLKIT_HOME=/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2
0: CRAY_CUDATOOLKIT_VERSION=23.9_12.2
1: CRAY_CUDATOOLKIT_POST_LINK_OPTS=-L/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2/lib64 -L/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2/nvvm/lib64 -L/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2/extras/Debugger/lib64 -L/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2/extras/CUPTI/lib64 -L/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/math_libs/12.2/lib64 -Wl,--as-needed,-lcupti,-lcudart,--no-as-needed -lcuda
1: CUDA_HOME=/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2
1: CRAY_CUDATOOLKIT_DIR=/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2
1: NVHPC_CUDA_HOME=/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2
1: CRAY_CUDATOOLKIT_INCLUDE_OPTS=-I/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2/include -I/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2/nvvm/include -I/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2/extras/Debugger/include -I/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2/extras/CUPTI/include -I/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/math_libs/12.2/include
1: CRAY_CUDATOOLKIT_PREFIX=/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2
1: LMOD_FAMILY_CUDATOOLKIT_VERSION=12.2
1: LMOD_FAMILY_CUDATOOLKIT=cudatoolkit
1: CUDA_VISIBLE_DEVICES=0
1: CUDATOOLKIT_HOME=/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2
1: CRAY_CUDATOOLKIT_VERSION=23.9_12.2
3: CRAY_CUDATOOLKIT_POST_LINK_OPTS=-L/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2/lib64 -L/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2/nvvm/lib64 -L/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2/extras/Debugger/lib64 -L/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2/extras/CUPTI/lib64 -L/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/math_libs/12.2/lib64 -Wl,--as-needed,-lcupti,-lcudart,--no-as-needed -lcuda
3: CUDA_HOME=/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2
3: CRAY_CUDATOOLKIT_DIR=/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2
3: NVHPC_CUDA_HOME=/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2
3: CRAY_CUDATOOLKIT_INCLUDE_OPTS=-I/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2/include -I/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2/nvvm/include -I/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2/extras/Debugger/include -I/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2/extras/CUPTI/include -I/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/math_libs/12.2/include
3: CRAY_CUDATOOLKIT_PREFIX=/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2
3: LMOD_FAMILY_CUDATOOLKIT_VERSION=12.2
3: LMOD_FAMILY_CUDATOOLKIT=cudatoolkit
3: CUDA_VISIBLE_DEVICES=0
3: CUDATOOLKIT_HOME=/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2
3: CRAY_CUDATOOLKIT_VERSION=23.9_12.2
2: CRAY_CUDATOOLKIT_POST_LINK_OPTS=-L/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2/lib64 -L/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2/nvvm/lib64 -L/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2/extras/Debugger/lib64 -L/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2/extras/CUPTI/lib64 -L/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/math_libs/12.2/lib64 -Wl,--as-needed,-lcupti,-lcudart,--no-as-needed -lcuda
2: CUDA_HOME=/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2
2: CRAY_CUDATOOLKIT_DIR=/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2
2: NVHPC_CUDA_HOME=/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2
2: CRAY_CUDATOOLKIT_INCLUDE_OPTS=-I/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2/include -I/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2/nvvm/include -I/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2/extras/Debugger/include -I/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2/extras/CUPTI/include -I/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/math_libs/12.2/include
2: CRAY_CUDATOOLKIT_PREFIX=/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2
2: LMOD_FAMILY_CUDATOOLKIT_VERSION=12.2
2: LMOD_FAMILY_CUDATOOLKIT=cudatoolkit
2: CUDA_VISIBLE_DEVICES=0
2: CUDATOOLKIT_HOME=/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2
2: CRAY_CUDATOOLKIT_VERSION=23.9_12.2
0: finished imports
0: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
0: random data shape:  torch.Size([1632, 80])
0: extra data shape:  torch.Size([1632, 80])
3: finished imports
3: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
3: random data shape:  torch.Size([1632, 80])
3: extra data shape:  torch.Size([1632, 80])
1: finished imports
1: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
1: random data shape:  torch.Size([1632, 80])
1: extra data shape:  torch.Size([1632, 80])
2: finished imports
2: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
2: random data shape:  torch.Size([1632, 80])
2: extra data shape:  torch.Size([1632, 80])
0: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_36000.csv
0: checking if experiment stats are in resutls file
0: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'act', 'ratio': 0.001, 'batch_size': 32, 'epochs': 1, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 36000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_36000.csv'}
0: This experiment exists:  True
1: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_108000.csv
1: checking if experiment stats are in resutls file
1: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'act', 'ratio': 0.001, 'batch_size': 32, 'epochs': 1, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 108000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_108000.csv'}
1: This experiment exists:  True
3: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_72000.csv
3: checking if experiment stats are in resutls file
3: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'act', 'ratio': 0.001, 'batch_size': 32, 'epochs': 1, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 72000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_72000.csv'}
3: This experiment exists:  True
2: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_143000.csv
2: checking if experiment stats are in resutls file
2: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'act', 'ratio': 0.001, 'batch_size': 32, 'epochs': 1, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 143000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_143000.csv'}
2: This experiment exists:  True
0: finished imports
0: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
0: random data shape:  torch.Size([1632, 80])
0: extra data shape:  torch.Size([1632, 80])
1: finished imports
1: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
1: random data shape:  torch.Size([1632, 80])
1: extra data shape:  torch.Size([1632, 80])
3: finished imports
3: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
3: random data shape:  torch.Size([1632, 80])
3: extra data shape:  torch.Size([1632, 80])
2: finished imports
2: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
2: random data shape:  torch.Size([1632, 80])
2: extra data shape:  torch.Size([1632, 80])
0: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_36000.csv
0: checking if experiment stats are in resutls file
0: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'act', 'ratio': 0.01, 'batch_size': 32, 'epochs': 1, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 36000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_36000.csv'}
0: This experiment exists:  True
1: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_108000.csv
1: checking if experiment stats are in resutls file
1: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'act', 'ratio': 0.01, 'batch_size': 32, 'epochs': 1, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 108000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_108000.csv'}
1: This experiment exists:  True
3: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_72000.csv
3: checking if experiment stats are in resutls file
3: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'act', 'ratio': 0.01, 'batch_size': 32, 'epochs': 1, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 72000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_72000.csv'}
3: This experiment exists:  True
2: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_143000.csv
2: checking if experiment stats are in resutls file
2: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'act', 'ratio': 0.01, 'batch_size': 32, 'epochs': 1, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 143000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_143000.csv'}
2: This experiment exists:  True
0: finished imports
0: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
0: random data shape:  torch.Size([1632, 80])
0: extra data shape:  torch.Size([1632, 80])
1: finished imports
1: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
1: random data shape:  torch.Size([1632, 80])
1: extra data shape:  torch.Size([1632, 80])
3: finished imports
3: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
3: random data shape:  torch.Size([1632, 80])
3: extra data shape:  torch.Size([1632, 80])
2: finished imports
2: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
2: random data shape:  torch.Size([1632, 80])
2: extra data shape:  torch.Size([1632, 80])
0: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_36000.csv
0: checking if experiment stats are in resutls file
0: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'act', 'ratio': 0.05, 'batch_size': 32, 'epochs': 1, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 36000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_36000.csv'}
0: This experiment exists:  True
1: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_108000.csv
1: checking if experiment stats are in resutls file
1: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'act', 'ratio': 0.05, 'batch_size': 32, 'epochs': 1, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 108000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_108000.csv'}
1: This experiment exists:  True
3: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_72000.csv
3: checking if experiment stats are in resutls file
3: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'act', 'ratio': 0.05, 'batch_size': 32, 'epochs': 1, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 72000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_72000.csv'}
3: This experiment exists:  True
2: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_143000.csv
2: checking if experiment stats are in resutls file
2: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'act', 'ratio': 0.05, 'batch_size': 32, 'epochs': 1, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 143000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_143000.csv'}
2: This experiment exists:  True
0: finished imports
0: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
0: random data shape:  torch.Size([1632, 80])
0: extra data shape:  torch.Size([1632, 80])
1: finished imports
1: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
1: random data shape:  torch.Size([1632, 80])
1: extra data shape:  torch.Size([1632, 80])
3: finished imports
3: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
3: random data shape:  torch.Size([1632, 80])
3: extra data shape:  torch.Size([1632, 80])
2: finished imports
2: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
2: random data shape:  torch.Size([1632, 80])
2: extra data shape:  torch.Size([1632, 80])
0: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_36000.csv
0: checking if experiment stats are in resutls file
0: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.001, 'batch_size': 32, 'epochs': 1, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 36000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_36000.csv'}
0: This experiment exists:  False
0: BEFORE MASKING---------
0: checking if experiment stats are in resutls file
0: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.001, 'batch_size': 32, 'epochs': 1, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 36000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_36000.csv'}
0: The basic stats exists:  True
0: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
0: Trainable Params:
0: gpt_neox.layers.0.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.1.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.2.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.3.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.4.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.5.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.6.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.7.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.8.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.9.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.10.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.11.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.12.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.13.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.14.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.15.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.16.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.17.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.18.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.19.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.20.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.21.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.22.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.23.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.24.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.25.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.26.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.27.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.28.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.29.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.30.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.31.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: ----------------------------------------------------------------------------------------------------
0: Applying ablation mask to model
0: Num of dropped neurons per layer:  16
0: model name:  EleutherAI/pythia-6.9b-deduped
1: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_108000.csv
1: checking if experiment stats are in resutls file
1: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.001, 'batch_size': 32, 'epochs': 1, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 108000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_108000.csv'}
1: This experiment exists:  False
1: BEFORE MASKING---------
1: checking if experiment stats are in resutls file
1: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.001, 'batch_size': 32, 'epochs': 1, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 108000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_108000.csv'}
1: The basic stats exists:  True
1: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
1: Trainable Params:
1: gpt_neox.layers.0.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.1.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.2.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.3.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.4.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.5.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.6.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.7.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.8.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.9.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.10.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.11.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.12.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.13.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.14.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.15.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.16.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.17.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.18.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.19.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.20.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.21.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.22.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.23.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.24.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.25.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.26.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.27.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.28.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.29.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.30.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.31.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: ----------------------------------------------------------------------------------------------------
1: Applying ablation mask to model
1: Num of dropped neurons per layer:  16
1: model name:  EleutherAI/pythia-6.9b-deduped
3: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_72000.csv
3: checking if experiment stats are in resutls file
3: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.001, 'batch_size': 32, 'epochs': 1, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 72000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_72000.csv'}
3: This experiment exists:  False
3: BEFORE MASKING---------
3: checking if experiment stats are in resutls file
3: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.001, 'batch_size': 32, 'epochs': 1, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 72000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_72000.csv'}
3: The basic stats exists:  True
3: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
3: Trainable Params:
3: gpt_neox.layers.0.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.1.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.2.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.3.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.4.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.5.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.6.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.7.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.8.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.9.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.10.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.11.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.12.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.13.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.14.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.15.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.16.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.17.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.18.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.19.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.20.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.21.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.22.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.23.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.24.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.25.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.26.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.27.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.28.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.29.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.30.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.31.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: ----------------------------------------------------------------------------------------------------
3: Applying ablation mask to model
3: Num of dropped neurons per layer:  16
3: model name:  EleutherAI/pythia-6.9b-deduped
2: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_143000.csv
2: checking if experiment stats are in resutls file
2: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.001, 'batch_size': 32, 'epochs': 1, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 143000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_143000.csv'}
2: This experiment exists:  False
2: BEFORE MASKING---------
2: checking if experiment stats are in resutls file
2: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.001, 'batch_size': 32, 'epochs': 1, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 143000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_143000.csv'}
2: The basic stats exists:  True
2: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
2: Trainable Params:
2: gpt_neox.layers.0.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.1.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.2.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.3.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.4.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.5.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.6.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.7.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.8.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.9.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.10.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.11.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.12.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.13.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.14.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.15.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.16.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.17.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.18.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.19.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.20.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.21.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.22.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.23.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.24.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.25.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.26.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.27.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.28.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.29.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.30.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.31.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: ----------------------------------------------------------------------------------------------------
2: Applying ablation mask to model
2: Num of dropped neurons per layer:  16
2: model name:  EleutherAI/pythia-6.9b-deduped
0: 
0:  AFTER MASKING Ablation---------
0: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.001/1/1000/0.1/0.1/pythia-6.9b-deduped
0: data shape:  torch.Size([505, 80])
0: checking perc mem
0: perc mem:  0.32673269510269165
0: perplexities of random pile batch:  23.54901960784314
0: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.001/1/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
0: appending only experiment not base results
0: appending to existing results file
0: finished imports
0: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
0: random data shape:  torch.Size([1632, 80])
0: extra data shape:  torch.Size([1632, 80])
2: 
2:  AFTER MASKING Ablation---------
2: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.001/1/1000/0.1/0.1/pythia-6.9b-deduped
2: data shape:  torch.Size([505, 80])
2: checking perc mem
3: 
3:  AFTER MASKING Ablation---------
3: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.001/1/1000/0.1/0.1/pythia-6.9b-deduped
3: data shape:  torch.Size([505, 80])
3: checking perc mem
1: 
1:  AFTER MASKING Ablation---------
1: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.001/1/1000/0.1/0.1/pythia-6.9b-deduped
1: data shape:  torch.Size([505, 80])
1: checking perc mem
2: perc mem:  0.7445544600486755
2: perplexities of random pile batch:  20.392156862745097
2: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.001/1/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
2: appending only experiment not base results
2: appending to existing results file
1: perc mem:  0.6198019981384277
1: perplexities of random pile batch:  20.627450980392158
1: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.001/1/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
1: appending only experiment not base results
1: appending to existing results file
3: perc mem:  0.5128713250160217
3: perplexities of random pile batch:  21.568627450980394
3: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.001/1/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
3: appending only experiment not base results
3: appending to existing results file
1: finished imports
1: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
1: random data shape:  torch.Size([1632, 80])
1: extra data shape:  torch.Size([1632, 80])
2: finished imports
2: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
2: random data shape:  torch.Size([1632, 80])
2: extra data shape:  torch.Size([1632, 80])
3: finished imports
3: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
3: random data shape:  torch.Size([1632, 80])
3: extra data shape:  torch.Size([1632, 80])
0: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_36000.csv
0: checking if experiment stats are in resutls file
0: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.001, 'batch_size': 32, 'epochs': 10, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 36000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_36000.csv'}
0: This experiment exists:  False
0: BEFORE MASKING---------
0: checking if experiment stats are in resutls file
0: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.001, 'batch_size': 32, 'epochs': 10, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 36000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_36000.csv'}
0: The basic stats exists:  True
0: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
0: Trainable Params:
0: gpt_neox.layers.0.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.1.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.2.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.3.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.4.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.5.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.6.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.7.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.8.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.9.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.10.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.11.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.12.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.13.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.14.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.15.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.16.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.17.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.18.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.19.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.20.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.21.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.22.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.23.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.24.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.25.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.26.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.27.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.28.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.29.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.30.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.31.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: ----------------------------------------------------------------------------------------------------
0: 10 lm loss: 8.383, reg_loss: 0.016
0:   Sparsity: 0.9974136352539062
0: Applying ablation mask to model
0: Num of dropped neurons per layer:  16
0: model name:  EleutherAI/pythia-6.9b-deduped
0: 
0:  AFTER MASKING Ablation---------
0: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.001/10/1000/0.1/0.1/pythia-6.9b-deduped
0: data shape:  torch.Size([505, 80])
0: checking perc mem
0: perc mem:  0.4019802212715149
0: perplexities of random pile batch:  23.235294117647058
0: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.001/10/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
0: appending only experiment not base results
0: appending to existing results file
1: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_108000.csv
1: checking if experiment stats are in resutls file
1: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.001, 'batch_size': 32, 'epochs': 10, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 108000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_108000.csv'}
1: This experiment exists:  False
1: BEFORE MASKING---------
1: checking if experiment stats are in resutls file
1: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.001, 'batch_size': 32, 'epochs': 10, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 108000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_108000.csv'}
1: The basic stats exists:  True
1: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
1: Trainable Params:
1: gpt_neox.layers.0.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.1.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.2.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.3.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.4.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.5.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.6.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.7.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.8.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.9.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.10.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.11.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.12.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.13.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.14.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.15.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.16.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.17.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.18.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.19.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.20.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.21.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.22.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.23.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.24.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.25.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.26.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.27.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.28.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.29.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.30.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.31.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: ----------------------------------------------------------------------------------------------------
1: 10 lm loss: 8.531, reg_loss: 0.016
1:   Sparsity: 0.9979877471923828
1: Applying ablation mask to model
1: Num of dropped neurons per layer:  16
1: model name:  EleutherAI/pythia-6.9b-deduped
2: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_143000.csv
2: checking if experiment stats are in resutls file
2: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.001, 'batch_size': 32, 'epochs': 10, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 143000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_143000.csv'}
2: This experiment exists:  False
2: BEFORE MASKING---------
2: checking if experiment stats are in resutls file
2: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.001, 'batch_size': 32, 'epochs': 10, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 143000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_143000.csv'}
2: The basic stats exists:  True
2: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
2: Trainable Params:
2: gpt_neox.layers.0.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.1.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.2.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.3.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.4.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.5.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.6.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.7.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.8.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.9.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.10.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.11.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.12.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.13.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.14.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.15.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.16.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.17.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.18.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.19.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.20.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.21.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.22.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.23.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.24.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.25.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.26.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.27.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.28.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.29.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.30.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.31.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: ----------------------------------------------------------------------------------------------------
2: 10 lm loss: 8.547, reg_loss: 0.016
2:   Sparsity: 0.997772216796875
2: Applying ablation mask to model
2: Num of dropped neurons per layer:  16
2: model name:  EleutherAI/pythia-6.9b-deduped
3: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_72000.csv
3: checking if experiment stats are in resutls file
3: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.001, 'batch_size': 32, 'epochs': 10, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 72000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_72000.csv'}
3: This experiment exists:  False
3: BEFORE MASKING---------
3: checking if experiment stats are in resutls file
3: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.001, 'batch_size': 32, 'epochs': 10, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 72000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_72000.csv'}
3: The basic stats exists:  True
3: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
3: Trainable Params:
3: gpt_neox.layers.0.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.1.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.2.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.3.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.4.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.5.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.6.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.7.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.8.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.9.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.10.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.11.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.12.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.13.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.14.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.15.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.16.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.17.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.18.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.19.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.20.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.21.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.22.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.23.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.24.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.25.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.26.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.27.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.28.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.29.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.30.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.31.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: ----------------------------------------------------------------------------------------------------
3: 10 lm loss: 8.203, reg_loss: 0.015
3:   Sparsity: 0.9981403350830078
3: Applying ablation mask to model
3: Num of dropped neurons per layer:  16
3: model name:  EleutherAI/pythia-6.9b-deduped
0: finished imports
0: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
0: random data shape:  torch.Size([1632, 80])
0: extra data shape:  torch.Size([1632, 80])
3: 
3:  AFTER MASKING Ablation---------
3: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.001/10/1000/0.1/0.1/pythia-6.9b-deduped
3: data shape:  torch.Size([505, 80])
3: checking perc mem
2: 
2:  AFTER MASKING Ablation---------
2: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.001/10/1000/0.1/0.1/pythia-6.9b-deduped
2: data shape:  torch.Size([505, 80])
2: checking perc mem
1: 
1:  AFTER MASKING Ablation---------
1: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.001/10/1000/0.1/0.1/pythia-6.9b-deduped
1: data shape:  torch.Size([505, 80])
1: checking perc mem
3: perc mem:  0.5405941009521484
3: perplexities of random pile batch:  21.647058823529413
3: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.001/10/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
3: appending only experiment not base results
3: appending to existing results file
2: perc mem:  0.8297029733657837
2: perplexities of random pile batch:  20.0
2: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.001/10/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
2: appending only experiment not base results
2: appending to existing results file
1: perc mem:  0.683168351650238
1: perplexities of random pile batch:  20.41176470588235
1: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.001/10/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
1: appending only experiment not base results
1: appending to existing results file
3: finished imports
3: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
3: random data shape:  torch.Size([1632, 80])
3: extra data shape:  torch.Size([1632, 80])
2: finished imports
2: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
2: random data shape:  torch.Size([1632, 80])
2: extra data shape:  torch.Size([1632, 80])
1: finished imports
1: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
1: random data shape:  torch.Size([1632, 80])
1: extra data shape:  torch.Size([1632, 80])
0: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_36000.csv
0: checking if experiment stats are in resutls file
0: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.001, 'batch_size': 32, 'epochs': 20, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 36000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_36000.csv'}
0: This experiment exists:  False
0: BEFORE MASKING---------
0: checking if experiment stats are in resutls file
0: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.001, 'batch_size': 32, 'epochs': 20, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 36000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_36000.csv'}
0: The basic stats exists:  True
0: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
0: Trainable Params:
0: gpt_neox.layers.0.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.1.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.2.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.3.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.4.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.5.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.6.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.7.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.8.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.9.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.10.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.11.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.12.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.13.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.14.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.15.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.16.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.17.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.18.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.19.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.20.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.21.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.22.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.23.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.24.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.25.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.26.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.27.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.28.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.29.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.30.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: gpt_neox.layers.31.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
0: ----------------------------------------------------------------------------------------------------
0: 10 lm loss: 8.383, reg_loss: 0.016
0:   Sparsity: 0.9974136352539062
0: 20 lm loss: 8.367, reg_loss: 0.007
0:   Sparsity: 0.99822998046875
0: Applying ablation mask to model
0: Num of dropped neurons per layer:  16
0: model name:  EleutherAI/pythia-6.9b-deduped
0: 
0:  AFTER MASKING Ablation---------
0: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.001/20/1000/0.1/0.1/pythia-6.9b-deduped
0: data shape:  torch.Size([505, 80])
0: checking perc mem
0: perc mem:  0.4178217947483063
0: perplexities of random pile batch:  23.235294117647058
0: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.001/20/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
0: appending only experiment not base results
0: appending to existing results file
0: finished imports
0: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
0: random data shape:  torch.Size([1632, 80])
0: extra data shape:  torch.Size([1632, 80])
0: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_36000.csv
0: checking if experiment stats are in resutls file
0: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.01, 'batch_size': 32, 'epochs': 1, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 36000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_36000.csv'}
0: This experiment exists:  False
0: BEFORE MASKING---------
0: checking if experiment stats are in resutls file
0: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.01, 'batch_size': 32, 'epochs': 1, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 36000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_36000.csv'}
0: The basic stats exists:  True
0: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
0: Loading pre-computed attributions.
0: Applying ablation mask to model
0: Num of dropped neurons per layer:  163
0: model name:  EleutherAI/pythia-6.9b-deduped
0: 
0:  AFTER MASKING Ablation---------
0: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.01/1/1000/0.1/0.1/pythia-6.9b-deduped
0: data shape:  torch.Size([505, 80])
0: checking perc mem
3: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_72000.csv
3: checking if experiment stats are in resutls file
3: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.001, 'batch_size': 32, 'epochs': 20, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 72000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_72000.csv'}
3: This experiment exists:  False
3: BEFORE MASKING---------
3: checking if experiment stats are in resutls file
3: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.001, 'batch_size': 32, 'epochs': 20, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 72000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_72000.csv'}
3: The basic stats exists:  True
3: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
3: Trainable Params:
3: gpt_neox.layers.0.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.1.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.2.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.3.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.4.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.5.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.6.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.7.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.8.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.9.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.10.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.11.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.12.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.13.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.14.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.15.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.16.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.17.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.18.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.19.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.20.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.21.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.22.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.23.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.24.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.25.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.26.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.27.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.28.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.29.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.30.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: gpt_neox.layers.31.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
3: ----------------------------------------------------------------------------------------------------
3: 10 lm loss: 8.203, reg_loss: 0.015
3:   Sparsity: 0.9981403350830078
3: 20 lm loss: 8.133, reg_loss: 0.007
3:   Sparsity: 0.9986629486083984
3: Applying ablation mask to model
3: Num of dropped neurons per layer:  16
3: model name:  EleutherAI/pythia-6.9b-deduped
2: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_143000.csv
2: checking if experiment stats are in resutls file
2: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.001, 'batch_size': 32, 'epochs': 20, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 143000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_143000.csv'}
2: This experiment exists:  False
2: BEFORE MASKING---------
2: checking if experiment stats are in resutls file
2: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.001, 'batch_size': 32, 'epochs': 20, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 143000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_143000.csv'}
2: The basic stats exists:  True
2: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
2: Trainable Params:
2: gpt_neox.layers.0.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.1.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.2.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.3.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.4.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.5.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.6.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.7.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.8.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.9.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.10.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.11.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.12.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.13.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.14.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.15.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.16.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.17.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.18.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.19.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.20.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.21.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.22.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.23.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.24.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.25.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.26.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.27.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.28.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.29.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.30.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: gpt_neox.layers.31.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
2: ----------------------------------------------------------------------------------------------------
2: 10 lm loss: 8.547, reg_loss: 0.016
2:   Sparsity: 0.997772216796875
2: 20 lm loss: 8.516, reg_loss: 0.007
2:   Sparsity: 0.9984874725341797
2: Applying ablation mask to model
2: Num of dropped neurons per layer:  16
2: model name:  EleutherAI/pythia-6.9b-deduped
1: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_108000.csv
1: checking if experiment stats are in resutls file
1: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.001, 'batch_size': 32, 'epochs': 20, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 108000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_108000.csv'}
1: This experiment exists:  False
1: BEFORE MASKING---------
1: checking if experiment stats are in resutls file
1: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.001, 'batch_size': 32, 'epochs': 20, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 108000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_108000.csv'}
1: The basic stats exists:  True
1: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
1: Trainable Params:
1: gpt_neox.layers.0.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.1.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.2.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.3.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.4.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.5.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.6.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.7.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.8.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.9.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.10.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.11.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.12.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.13.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.14.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.15.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.16.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.17.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.18.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.19.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.20.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.21.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.22.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.23.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.24.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.25.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.26.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.27.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.28.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.29.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.30.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: gpt_neox.layers.31.mlp.dense_4h_to_h.mask.mask_scores torch.Size([1, 1, 1, 16384])
1: ----------------------------------------------------------------------------------------------------
1: 10 lm loss: 8.531, reg_loss: 0.016
1:   Sparsity: 0.9979877471923828
1: 20 lm loss: 8.477, reg_loss: 0.007
1:   Sparsity: 0.9989376068115234
1: Applying ablation mask to model
1: Num of dropped neurons per layer:  16
1: model name:  EleutherAI/pythia-6.9b-deduped
0: perc mem:  0.2415841668844223
0: perplexities of random pile batch:  26.07843137254902
0: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.01/1/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
0: appending only experiment not base results
0: appending to existing results file
0: finished imports
0: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
0: random data shape:  torch.Size([1632, 80])
0: extra data shape:  torch.Size([1632, 80])
0: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_36000.csv
0: checking if experiment stats are in resutls file
0: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.01, 'batch_size': 32, 'epochs': 10, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 36000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_36000.csv'}
0: This experiment exists:  False
0: BEFORE MASKING---------
0: checking if experiment stats are in resutls file
0: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.01, 'batch_size': 32, 'epochs': 10, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 36000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_36000.csv'}
0: The basic stats exists:  True
0: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
0: Loading pre-computed attributions.
0: Applying ablation mask to model
0: Num of dropped neurons per layer:  163
0: model name:  EleutherAI/pythia-6.9b-deduped
0: 
0:  AFTER MASKING Ablation---------
0: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.01/10/1000/0.1/0.1/pythia-6.9b-deduped
0: data shape:  torch.Size([505, 80])
0: checking perc mem
0: perc mem:  0.22574257850646973
0: perplexities of random pile batch:  26.352941176470587
0: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.01/10/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
0: appending only experiment not base results
0: appending to existing results file
0: finished imports
0: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
0: random data shape:  torch.Size([1632, 80])
0: extra data shape:  torch.Size([1632, 80])
2: 
2:  AFTER MASKING Ablation---------
2: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.001/20/1000/0.1/0.1/pythia-6.9b-deduped
2: data shape:  torch.Size([505, 80])
2: checking perc mem
0: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_36000.csv
0: checking if experiment stats are in resutls file
0: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.01, 'batch_size': 32, 'epochs': 20, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 36000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_36000.csv'}
0: This experiment exists:  False
0: BEFORE MASKING---------
0: checking if experiment stats are in resutls file
0: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.01, 'batch_size': 32, 'epochs': 20, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 36000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_36000.csv'}
0: The basic stats exists:  True
0: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
0: Loading pre-computed attributions.
0: Applying ablation mask to model
0: Num of dropped neurons per layer:  163
0: model name:  EleutherAI/pythia-6.9b-deduped
3: 
3:  AFTER MASKING Ablation---------
3: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.001/20/1000/0.1/0.1/pythia-6.9b-deduped
3: data shape:  torch.Size([505, 80])
3: checking perc mem
1: 
1:  AFTER MASKING Ablation---------
1: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.001/20/1000/0.1/0.1/pythia-6.9b-deduped
1: data shape:  torch.Size([505, 80])
1: checking perc mem
2: perc mem:  0.8158416152000427
2: perplexities of random pile batch:  20.313725490196077
2: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.001/20/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
2: appending only experiment not base results
2: appending to existing results file
3: perc mem:  0.5445544719696045
3: perplexities of random pile batch:  21.686274509803923
3: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.001/20/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
3: appending only experiment not base results
3: appending to existing results file
1: perc mem:  0.6653465628623962
1: perplexities of random pile batch:  20.54901960784314
1: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.001/20/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
1: appending only experiment not base results
1: appending to existing results file
0: 
0:  AFTER MASKING Ablation---------
0: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.01/20/1000/0.1/0.1/pythia-6.9b-deduped
0: data shape:  torch.Size([505, 80])
0: checking perc mem
2: finished imports
2: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
2: random data shape:  torch.Size([1632, 80])
2: extra data shape:  torch.Size([1632, 80])
3: finished imports
3: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
3: random data shape:  torch.Size([1632, 80])
3: extra data shape:  torch.Size([1632, 80])
1: finished imports
1: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
1: random data shape:  torch.Size([1632, 80])
1: extra data shape:  torch.Size([1632, 80])
2: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_143000.csv
2: checking if experiment stats are in resutls file
2: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.01, 'batch_size': 32, 'epochs': 1, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 143000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_143000.csv'}
2: This experiment exists:  False
2: BEFORE MASKING---------
2: checking if experiment stats are in resutls file
2: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.01, 'batch_size': 32, 'epochs': 1, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 143000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_143000.csv'}
2: The basic stats exists:  True
2: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
2: Loading pre-computed attributions.
2: Applying ablation mask to model
2: Num of dropped neurons per layer:  163
2: model name:  EleutherAI/pythia-6.9b-deduped
3: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_72000.csv
3: checking if experiment stats are in resutls file
3: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.01, 'batch_size': 32, 'epochs': 1, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 72000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_72000.csv'}
3: This experiment exists:  False
3: BEFORE MASKING---------
3: checking if experiment stats are in resutls file
3: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.01, 'batch_size': 32, 'epochs': 1, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 72000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_72000.csv'}
3: The basic stats exists:  True
3: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
3: Loading pre-computed attributions.
3: Applying ablation mask to model
3: Num of dropped neurons per layer:  163
3: model name:  EleutherAI/pythia-6.9b-deduped
1: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_108000.csv
1: checking if experiment stats are in resutls file
1: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.01, 'batch_size': 32, 'epochs': 1, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 108000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_108000.csv'}
1: This experiment exists:  False
1: BEFORE MASKING---------
1: checking if experiment stats are in resutls file
1: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.01, 'batch_size': 32, 'epochs': 1, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 108000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_108000.csv'}
1: The basic stats exists:  True
1: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
1: Loading pre-computed attributions.
1: Applying ablation mask to model
1: Num of dropped neurons per layer:  163
1: model name:  EleutherAI/pythia-6.9b-deduped
0: perc mem:  0.21188119053840637
0: perplexities of random pile batch:  26.294117647058822
0: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.01/20/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
0: appending only experiment not base results
0: appending to existing results file
0: finished imports
0: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
0: random data shape:  torch.Size([1632, 80])
0: extra data shape:  torch.Size([1632, 80])
0: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_36000.csv
0: checking if experiment stats are in resutls file
0: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.05, 'batch_size': 32, 'epochs': 1, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 36000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_36000.csv'}
0: This experiment exists:  False
0: BEFORE MASKING---------
0: checking if experiment stats are in resutls file
0: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.05, 'batch_size': 32, 'epochs': 1, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 36000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_36000.csv'}
0: The basic stats exists:  True
0: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
0: Loading pre-computed attributions.
0: Applying ablation mask to model
0: Num of dropped neurons per layer:  819
0: model name:  EleutherAI/pythia-6.9b-deduped
0: 
0:  AFTER MASKING Ablation---------
0: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.05/1/1000/0.1/0.1/pythia-6.9b-deduped
0: data shape:  torch.Size([505, 80])
0: checking perc mem
3: 
3:  AFTER MASKING Ablation---------
3: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.01/1/1000/0.1/0.1/pythia-6.9b-deduped
3: data shape:  torch.Size([505, 80])
3: checking perc mem
1: 
1:  AFTER MASKING Ablation---------
1: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.01/1/1000/0.1/0.1/pythia-6.9b-deduped
1: data shape:  torch.Size([505, 80])
1: checking perc mem
2: 
2:  AFTER MASKING Ablation---------
2: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.01/1/1000/0.1/0.1/pythia-6.9b-deduped
2: data shape:  torch.Size([505, 80])
2: checking perc mem
0: perc mem:  0.19207921624183655
0: perplexities of random pile batch:  27.176470588235293
0: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.05/1/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
0: appending only experiment not base results
0: appending to existing results file
0: finished imports
0: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
0: random data shape:  torch.Size([1632, 80])
0: extra data shape:  torch.Size([1632, 80])
0: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_36000.csv
0: checking if experiment stats are in resutls file
0: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.05, 'batch_size': 32, 'epochs': 10, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 36000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_36000.csv'}
0: This experiment exists:  False
0: BEFORE MASKING---------
0: checking if experiment stats are in resutls file
0: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.05, 'batch_size': 32, 'epochs': 10, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 36000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_36000.csv'}
0: The basic stats exists:  True
0: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
0: Loading pre-computed attributions.
0: Applying ablation mask to model
0: Num of dropped neurons per layer:  819
0: model name:  EleutherAI/pythia-6.9b-deduped
3: perc mem:  0.3465346693992615
3: perplexities of random pile batch:  24.058823529411764
3: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.01/1/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
3: appending only experiment not base results
3: appending to existing results file
1: perc mem:  0.4099009931087494
1: perplexities of random pile batch:  23.41176470588235
1: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.01/1/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
1: appending only experiment not base results
1: appending to existing results file
2: perc mem:  0.41386139392852783
2: perplexities of random pile batch:  23.54901960784314
2: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.01/1/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
2: appending only experiment not base results
2: appending to existing results file
3: finished imports
3: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
3: random data shape:  torch.Size([1632, 80])
3: extra data shape:  torch.Size([1632, 80])
1: finished imports
1: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
1: random data shape:  torch.Size([1632, 80])
1: extra data shape:  torch.Size([1632, 80])
2: finished imports
2: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
2: random data shape:  torch.Size([1632, 80])
2: extra data shape:  torch.Size([1632, 80])
3: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_72000.csv
3: checking if experiment stats are in resutls file
3: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.01, 'batch_size': 32, 'epochs': 10, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 72000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_72000.csv'}
3: This experiment exists:  False
3: BEFORE MASKING---------
3: checking if experiment stats are in resutls file
3: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.01, 'batch_size': 32, 'epochs': 10, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 72000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_72000.csv'}
3: The basic stats exists:  True
3: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
3: Loading pre-computed attributions.
3: Applying ablation mask to model
3: Num of dropped neurons per layer:  163
3: model name:  EleutherAI/pythia-6.9b-deduped
1: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_108000.csv
1: checking if experiment stats are in resutls file
1: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.01, 'batch_size': 32, 'epochs': 10, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 108000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_108000.csv'}
1: This experiment exists:  False
1: BEFORE MASKING---------
1: checking if experiment stats are in resutls file
1: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.01, 'batch_size': 32, 'epochs': 10, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 108000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_108000.csv'}
1: The basic stats exists:  True
1: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
1: Loading pre-computed attributions.
1: Applying ablation mask to model
1: Num of dropped neurons per layer:  163
1: model name:  EleutherAI/pythia-6.9b-deduped
2: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_143000.csv
2: checking if experiment stats are in resutls file
2: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.01, 'batch_size': 32, 'epochs': 10, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 143000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_143000.csv'}
2: This experiment exists:  False
2: BEFORE MASKING---------
2: checking if experiment stats are in resutls file
2: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.01, 'batch_size': 32, 'epochs': 10, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 143000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_143000.csv'}
2: The basic stats exists:  True
2: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
2: Loading pre-computed attributions.
2: Applying ablation mask to model
2: Num of dropped neurons per layer:  163
2: model name:  EleutherAI/pythia-6.9b-deduped
0: 
0:  AFTER MASKING Ablation---------
0: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.05/10/1000/0.1/0.1/pythia-6.9b-deduped
0: data shape:  torch.Size([505, 80])
0: checking perc mem
0: perc mem:  0.13861386477947235
0: perplexities of random pile batch:  27.50980392156863
0: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.05/10/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
0: appending only experiment not base results
0: appending to existing results file
0: finished imports
0: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
0: random data shape:  torch.Size([1632, 80])
0: extra data shape:  torch.Size([1632, 80])
0: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_36000.csv
0: checking if experiment stats are in resutls file
0: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.05, 'batch_size': 32, 'epochs': 20, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 36000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_36000.csv'}
0: This experiment exists:  False
0: BEFORE MASKING---------
0: checking if experiment stats are in resutls file
0: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.05, 'batch_size': 32, 'epochs': 20, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 36000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_36000.csv'}
0: The basic stats exists:  True
0: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
0: Loading pre-computed attributions.
0: Applying ablation mask to model
0: Num of dropped neurons per layer:  819
0: model name:  EleutherAI/pythia-6.9b-deduped
0: 
0:  AFTER MASKING Ablation---------
0: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.05/20/1000/0.1/0.1/pythia-6.9b-deduped
0: data shape:  torch.Size([505, 80])
0: checking perc mem
1: 
1:  AFTER MASKING Ablation---------
1: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.01/10/1000/0.1/0.1/pythia-6.9b-deduped
1: data shape:  torch.Size([505, 80])
1: checking perc mem
3: 
3:  AFTER MASKING Ablation---------
3: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.01/10/1000/0.1/0.1/pythia-6.9b-deduped
3: data shape:  torch.Size([505, 80])
3: checking perc mem
2: 
2:  AFTER MASKING Ablation---------
2: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.01/10/1000/0.1/0.1/pythia-6.9b-deduped
2: data shape:  torch.Size([505, 80])
2: checking perc mem
0: perc mem:  0.13663366436958313
0: perplexities of random pile batch:  27.352941176470587
0: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.05/20/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
0: appending only experiment not base results
0: appending to existing results file
1: perc mem:  0.28910893201828003
1: perplexities of random pile batch:  23.96078431372549
1: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.01/10/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
1: appending only experiment not base results
1: appending to existing results file
3: perc mem:  0.2752475440502167
3: perplexities of random pile batch:  24.41176470588235
3: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.01/10/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
3: appending only experiment not base results
3: appending to existing results file
2: perc mem:  0.28316831588745117
2: perplexities of random pile batch:  24.313725490196077
2: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.01/10/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
2: appending only experiment not base results
2: appending to existing results file
0: finished imports
0: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
0: random data shape:  torch.Size([1632, 80])
0: extra data shape:  torch.Size([1632, 80])
1: finished imports
1: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
1: random data shape:  torch.Size([1632, 80])
1: extra data shape:  torch.Size([1632, 80])
3: finished imports
3: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
3: random data shape:  torch.Size([1632, 80])
3: extra data shape:  torch.Size([1632, 80])
2: finished imports
2: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
2: random data shape:  torch.Size([1632, 80])
2: extra data shape:  torch.Size([1632, 80])
0: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_36000.csv
0: checking if experiment stats are in resutls file
0: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.001, 'batch_size': 32, 'epochs': 1, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 36000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_36000.csv'}
0: This experiment exists:  False
0: BEFORE MASKING---------
0: checking if experiment stats are in resutls file
0: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.001, 'batch_size': 32, 'epochs': 1, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 36000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_36000.csv'}
0: The basic stats exists:  True
0: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
0: starting slim localization
0: gpt_neox.layers.0.mlp.act.slim_coef
0: gpt_neox.layers.1.mlp.act.slim_coef
0: gpt_neox.layers.2.mlp.act.slim_coef
0: gpt_neox.layers.3.mlp.act.slim_coef
0: gpt_neox.layers.4.mlp.act.slim_coef
0: gpt_neox.layers.5.mlp.act.slim_coef
0: gpt_neox.layers.6.mlp.act.slim_coef
0: gpt_neox.layers.7.mlp.act.slim_coef
0: gpt_neox.layers.8.mlp.act.slim_coef
0: gpt_neox.layers.9.mlp.act.slim_coef
0: gpt_neox.layers.10.mlp.act.slim_coef
0: gpt_neox.layers.11.mlp.act.slim_coef
0: gpt_neox.layers.12.mlp.act.slim_coef
0: gpt_neox.layers.13.mlp.act.slim_coef
0: gpt_neox.layers.14.mlp.act.slim_coef
0: gpt_neox.layers.15.mlp.act.slim_coef
0: gpt_neox.layers.16.mlp.act.slim_coef
0: gpt_neox.layers.17.mlp.act.slim_coef
0: gpt_neox.layers.18.mlp.act.slim_coef
0: gpt_neox.layers.19.mlp.act.slim_coef
0: gpt_neox.layers.20.mlp.act.slim_coef
0: gpt_neox.layers.21.mlp.act.slim_coef
0: gpt_neox.layers.22.mlp.act.slim_coef
0: gpt_neox.layers.23.mlp.act.slim_coef
0: gpt_neox.layers.24.mlp.act.slim_coef
0: gpt_neox.layers.25.mlp.act.slim_coef
0: gpt_neox.layers.26.mlp.act.slim_coef
0: gpt_neox.layers.27.mlp.act.slim_coef
0: gpt_neox.layers.28.mlp.act.slim_coef
0: gpt_neox.layers.29.mlp.act.slim_coef
0: gpt_neox.layers.30.mlp.act.slim_coef
0: gpt_neox.layers.31.mlp.act.slim_coef
0: ----------------------------------------------------------------------------------------------------
1: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_108000.csv
1: checking if experiment stats are in resutls file
1: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.01, 'batch_size': 32, 'epochs': 20, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 108000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_108000.csv'}
1: This experiment exists:  False
1: BEFORE MASKING---------
1: checking if experiment stats are in resutls file
1: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.01, 'batch_size': 32, 'epochs': 20, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 108000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_108000.csv'}
1: The basic stats exists:  True
1: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
1: Loading pre-computed attributions.
1: Applying ablation mask to model
1: Num of dropped neurons per layer:  163
1: model name:  EleutherAI/pythia-6.9b-deduped
3: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_72000.csv
3: checking if experiment stats are in resutls file
3: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.01, 'batch_size': 32, 'epochs': 20, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 72000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_72000.csv'}
3: This experiment exists:  False
3: BEFORE MASKING---------
3: checking if experiment stats are in resutls file
3: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.01, 'batch_size': 32, 'epochs': 20, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 72000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_72000.csv'}
3: The basic stats exists:  True
3: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
3: Loading pre-computed attributions.
3: Applying ablation mask to model
3: Num of dropped neurons per layer:  163
3: model name:  EleutherAI/pythia-6.9b-deduped
2: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_143000.csv
2: checking if experiment stats are in resutls file
2: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.01, 'batch_size': 32, 'epochs': 20, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 143000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_143000.csv'}
2: This experiment exists:  False
2: BEFORE MASKING---------
2: checking if experiment stats are in resutls file
2: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.01, 'batch_size': 32, 'epochs': 20, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 143000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_143000.csv'}
2: The basic stats exists:  True
2: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
2: Loading pre-computed attributions.
2: Applying ablation mask to model
2: Num of dropped neurons per layer:  163
2: model name:  EleutherAI/pythia-6.9b-deduped
0: Applying ablation mask to model
0: Num of dropped neurons per layer:  16
0: model name:  EleutherAI/pythia-6.9b-deduped
0: 
0:  AFTER MASKING Ablation---------
0: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.001/1/1000/0.1/0.1/pythia-6.9b-deduped
0: data shape:  torch.Size([505, 80])
0: checking perc mem
1: 
1:  AFTER MASKING Ablation---------
1: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.01/20/1000/0.1/0.1/pythia-6.9b-deduped
1: data shape:  torch.Size([505, 80])
1: checking perc mem
3: 
3:  AFTER MASKING Ablation---------
3: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.01/20/1000/0.1/0.1/pythia-6.9b-deduped
3: data shape:  torch.Size([505, 80])
3: checking perc mem
2: 
2:  AFTER MASKING Ablation---------
2: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.01/20/1000/0.1/0.1/pythia-6.9b-deduped
2: data shape:  torch.Size([505, 80])
2: checking perc mem
0: perc mem:  0.42376238107681274
0: perplexities of random pile batch:  22.901960784313726
0: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.001/1/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
0: appending only experiment not base results
0: appending to existing results file
1: perc mem:  0.32673269510269165
1: perplexities of random pile batch:  23.666666666666668
1: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.01/20/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
1: appending only experiment not base results
1: appending to existing results file
1: finished imports
1: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
1: random data shape:  torch.Size([1632, 80])
1: extra data shape:  torch.Size([1632, 80])
3: perc mem:  0.27326732873916626
3: perplexities of random pile batch:  24.274509803921568
3: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.01/20/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
3: appending only experiment not base results
3: appending to existing results file
0: finished imports
0: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
0: random data shape:  torch.Size([1632, 80])
0: extra data shape:  torch.Size([1632, 80])
2: perc mem:  0.3287128806114197
2: perplexities of random pile batch:  23.823529411764707
2: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.01/20/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
2: appending only experiment not base results
2: appending to existing results file
1: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_108000.csv
1: checking if experiment stats are in resutls file
1: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.05, 'batch_size': 32, 'epochs': 1, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 108000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_108000.csv'}
1: This experiment exists:  False
1: BEFORE MASKING---------
1: checking if experiment stats are in resutls file
1: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.05, 'batch_size': 32, 'epochs': 1, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 108000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_108000.csv'}
1: The basic stats exists:  True
1: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
1: Loading pre-computed attributions.
1: Applying ablation mask to model
1: Num of dropped neurons per layer:  819
1: model name:  EleutherAI/pythia-6.9b-deduped
3: finished imports
3: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
3: random data shape:  torch.Size([1632, 80])
3: extra data shape:  torch.Size([1632, 80])
2: finished imports
2: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
2: random data shape:  torch.Size([1632, 80])
2: extra data shape:  torch.Size([1632, 80])
3: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_72000.csv
3: checking if experiment stats are in resutls file
3: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.05, 'batch_size': 32, 'epochs': 1, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 72000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_72000.csv'}
3: This experiment exists:  False
3: BEFORE MASKING---------
3: checking if experiment stats are in resutls file
3: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.05, 'batch_size': 32, 'epochs': 1, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 72000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_72000.csv'}
3: The basic stats exists:  True
3: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
3: Loading pre-computed attributions.
3: Applying ablation mask to model
3: Num of dropped neurons per layer:  819
3: model name:  EleutherAI/pythia-6.9b-deduped
2: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_143000.csv
2: checking if experiment stats are in resutls file
2: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.05, 'batch_size': 32, 'epochs': 1, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 143000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_143000.csv'}
2: This experiment exists:  False
2: BEFORE MASKING---------
2: checking if experiment stats are in resutls file
2: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.05, 'batch_size': 32, 'epochs': 1, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 143000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_143000.csv'}
2: The basic stats exists:  True
2: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
2: Loading pre-computed attributions.
2: Applying ablation mask to model
2: Num of dropped neurons per layer:  819
2: model name:  EleutherAI/pythia-6.9b-deduped
1: 
1:  AFTER MASKING Ablation---------
1: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.05/1/1000/0.1/0.1/pythia-6.9b-deduped
1: data shape:  torch.Size([505, 80])
1: checking perc mem
0: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_36000.csv
0: checking if experiment stats are in resutls file
0: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.001, 'batch_size': 32, 'epochs': 10, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 36000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_36000.csv'}
0: This experiment exists:  False
0: BEFORE MASKING---------
0: checking if experiment stats are in resutls file
0: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.001, 'batch_size': 32, 'epochs': 10, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 36000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_36000.csv'}
0: The basic stats exists:  True
0: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
0: starting slim localization
0: gpt_neox.layers.0.mlp.act.slim_coef
0: gpt_neox.layers.1.mlp.act.slim_coef
0: gpt_neox.layers.2.mlp.act.slim_coef
0: gpt_neox.layers.3.mlp.act.slim_coef
0: gpt_neox.layers.4.mlp.act.slim_coef
0: gpt_neox.layers.5.mlp.act.slim_coef
0: gpt_neox.layers.6.mlp.act.slim_coef
0: gpt_neox.layers.7.mlp.act.slim_coef
0: gpt_neox.layers.8.mlp.act.slim_coef
0: gpt_neox.layers.9.mlp.act.slim_coef
0: gpt_neox.layers.10.mlp.act.slim_coef
0: gpt_neox.layers.11.mlp.act.slim_coef
0: gpt_neox.layers.12.mlp.act.slim_coef
0: gpt_neox.layers.13.mlp.act.slim_coef
0: gpt_neox.layers.14.mlp.act.slim_coef
0: gpt_neox.layers.15.mlp.act.slim_coef
0: gpt_neox.layers.16.mlp.act.slim_coef
0: gpt_neox.layers.17.mlp.act.slim_coef
0: gpt_neox.layers.18.mlp.act.slim_coef
0: gpt_neox.layers.19.mlp.act.slim_coef
0: gpt_neox.layers.20.mlp.act.slim_coef
0: gpt_neox.layers.21.mlp.act.slim_coef
0: gpt_neox.layers.22.mlp.act.slim_coef
0: gpt_neox.layers.23.mlp.act.slim_coef
0: gpt_neox.layers.24.mlp.act.slim_coef
0: gpt_neox.layers.25.mlp.act.slim_coef
0: gpt_neox.layers.26.mlp.act.slim_coef
0: gpt_neox.layers.27.mlp.act.slim_coef
0: gpt_neox.layers.28.mlp.act.slim_coef
0: gpt_neox.layers.29.mlp.act.slim_coef
0: gpt_neox.layers.30.mlp.act.slim_coef
0: gpt_neox.layers.31.mlp.act.slim_coef
0: ----------------------------------------------------------------------------------------------------
1: perc mem:  0.15049505233764648
1: perplexities of random pile batch:  28.15686274509804
1: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.05/1/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
1: appending only experiment not base results
1: appending to existing results file
1: finished imports
1: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
1: random data shape:  torch.Size([1632, 80])
1: extra data shape:  torch.Size([1632, 80])
2: 
2:  AFTER MASKING Ablation---------
2: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.05/1/1000/0.1/0.1/pythia-6.9b-deduped
2: data shape:  torch.Size([505, 80])
2: checking perc mem
3: 
3:  AFTER MASKING Ablation---------
3: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.05/1/1000/0.1/0.1/pythia-6.9b-deduped
3: data shape:  torch.Size([505, 80])
3: checking perc mem
1: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_108000.csv
1: checking if experiment stats are in resutls file
1: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.05, 'batch_size': 32, 'epochs': 10, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 108000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_108000.csv'}
1: This experiment exists:  False
1: BEFORE MASKING---------
1: checking if experiment stats are in resutls file
1: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.05, 'batch_size': 32, 'epochs': 10, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 108000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_108000.csv'}
1: The basic stats exists:  True
1: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
1: Loading pre-computed attributions.
1: Applying ablation mask to model
1: Num of dropped neurons per layer:  819
1: model name:  EleutherAI/pythia-6.9b-deduped
0: Applying ablation mask to model
0: Num of dropped neurons per layer:  16
0: model name:  EleutherAI/pythia-6.9b-deduped
2: perc mem:  0.10297030210494995
2: perplexities of random pile batch:  31.607843137254903
2: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.05/1/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
2: appending only experiment not base results
2: appending to existing results file
3: perc mem:  0.196039617061615
3: perplexities of random pile batch:  25.96078431372549
3: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.05/1/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
3: appending only experiment not base results
3: appending to existing results file
1: 
1:  AFTER MASKING Ablation---------
1: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.05/10/1000/0.1/0.1/pythia-6.9b-deduped
1: data shape:  torch.Size([505, 80])
1: checking perc mem
2: finished imports
2: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
2: random data shape:  torch.Size([1632, 80])
2: extra data shape:  torch.Size([1632, 80])
3: finished imports
3: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
3: random data shape:  torch.Size([1632, 80])
3: extra data shape:  torch.Size([1632, 80])
2: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_143000.csv
2: checking if experiment stats are in resutls file
2: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.05, 'batch_size': 32, 'epochs': 10, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 143000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_143000.csv'}
2: This experiment exists:  False
2: BEFORE MASKING---------
2: checking if experiment stats are in resutls file
2: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.05, 'batch_size': 32, 'epochs': 10, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 143000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_143000.csv'}
2: The basic stats exists:  True
2: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
2: Loading pre-computed attributions.
2: Applying ablation mask to model
2: Num of dropped neurons per layer:  819
2: model name:  EleutherAI/pythia-6.9b-deduped
3: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_72000.csv
3: checking if experiment stats are in resutls file
3: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.05, 'batch_size': 32, 'epochs': 10, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 72000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_72000.csv'}
3: This experiment exists:  False
3: BEFORE MASKING---------
3: checking if experiment stats are in resutls file
3: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.05, 'batch_size': 32, 'epochs': 10, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 72000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_72000.csv'}
3: The basic stats exists:  True
3: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
3: Loading pre-computed attributions.
3: Applying ablation mask to model
3: Num of dropped neurons per layer:  819
3: model name:  EleutherAI/pythia-6.9b-deduped
0: 
0:  AFTER MASKING Ablation---------
0: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.001/10/1000/0.1/0.1/pythia-6.9b-deduped
0: data shape:  torch.Size([505, 80])
0: checking perc mem
1: perc mem:  0.13663366436958313
1: perplexities of random pile batch:  26.098039215686274
1: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.05/10/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
1: appending only experiment not base results
1: appending to existing results file
1: finished imports
1: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
1: random data shape:  torch.Size([1632, 80])
1: extra data shape:  torch.Size([1632, 80])
1: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_108000.csv
1: checking if experiment stats are in resutls file
1: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.05, 'batch_size': 32, 'epochs': 20, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 108000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_108000.csv'}
1: This experiment exists:  False
1: BEFORE MASKING---------
1: checking if experiment stats are in resutls file
1: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.05, 'batch_size': 32, 'epochs': 20, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 108000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_108000.csv'}
1: The basic stats exists:  True
1: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
1: Loading pre-computed attributions.
1: Applying ablation mask to model
1: Num of dropped neurons per layer:  819
1: model name:  EleutherAI/pythia-6.9b-deduped
0: perc mem:  0.42376238107681274
0: perplexities of random pile batch:  22.901960784313726
0: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.001/10/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
0: appending only experiment not base results
0: appending to existing results file
2: 
2:  AFTER MASKING Ablation---------
2: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.05/10/1000/0.1/0.1/pythia-6.9b-deduped
2: data shape:  torch.Size([505, 80])
2: checking perc mem
0: finished imports
0: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
0: random data shape:  torch.Size([1632, 80])
0: extra data shape:  torch.Size([1632, 80])
3: 
3:  AFTER MASKING Ablation---------
3: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.05/10/1000/0.1/0.1/pythia-6.9b-deduped
3: data shape:  torch.Size([505, 80])
3: checking perc mem
0: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_36000.csv
0: checking if experiment stats are in resutls file
0: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.001, 'batch_size': 32, 'epochs': 20, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 36000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_36000.csv'}
0: This experiment exists:  False
0: BEFORE MASKING---------
0: checking if experiment stats are in resutls file
0: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.001, 'batch_size': 32, 'epochs': 20, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 36000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_36000.csv'}
0: The basic stats exists:  True
0: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
0: starting slim localization
0: gpt_neox.layers.0.mlp.act.slim_coef
0: gpt_neox.layers.1.mlp.act.slim_coef
0: gpt_neox.layers.2.mlp.act.slim_coef
0: gpt_neox.layers.3.mlp.act.slim_coef
0: gpt_neox.layers.4.mlp.act.slim_coef
0: gpt_neox.layers.5.mlp.act.slim_coef
0: gpt_neox.layers.6.mlp.act.slim_coef
0: gpt_neox.layers.7.mlp.act.slim_coef
0: gpt_neox.layers.8.mlp.act.slim_coef
0: gpt_neox.layers.9.mlp.act.slim_coef
0: gpt_neox.layers.10.mlp.act.slim_coef
0: gpt_neox.layers.11.mlp.act.slim_coef
0: gpt_neox.layers.12.mlp.act.slim_coef
0: gpt_neox.layers.13.mlp.act.slim_coef
0: gpt_neox.layers.14.mlp.act.slim_coef
0: gpt_neox.layers.15.mlp.act.slim_coef
0: gpt_neox.layers.16.mlp.act.slim_coef
0: gpt_neox.layers.17.mlp.act.slim_coef
0: gpt_neox.layers.18.mlp.act.slim_coef
0: gpt_neox.layers.19.mlp.act.slim_coef
0: gpt_neox.layers.20.mlp.act.slim_coef
0: gpt_neox.layers.21.mlp.act.slim_coef
0: gpt_neox.layers.22.mlp.act.slim_coef
0: gpt_neox.layers.23.mlp.act.slim_coef
0: gpt_neox.layers.24.mlp.act.slim_coef
0: gpt_neox.layers.25.mlp.act.slim_coef
0: gpt_neox.layers.26.mlp.act.slim_coef
0: gpt_neox.layers.27.mlp.act.slim_coef
0: gpt_neox.layers.28.mlp.act.slim_coef
0: gpt_neox.layers.29.mlp.act.slim_coef
0: gpt_neox.layers.30.mlp.act.slim_coef
0: gpt_neox.layers.31.mlp.act.slim_coef
0: ----------------------------------------------------------------------------------------------------
1: 
1:  AFTER MASKING Ablation---------
1: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.05/20/1000/0.1/0.1/pythia-6.9b-deduped
1: data shape:  torch.Size([505, 80])
1: checking perc mem
2: perc mem:  0.13861386477947235
2: perplexities of random pile batch:  27.0
2: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.05/10/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
2: appending only experiment not base results
2: appending to existing results file
3: perc mem:  0.15841585397720337
3: perplexities of random pile batch:  26.07843137254902
3: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.05/10/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
3: appending only experiment not base results
3: appending to existing results file
2: finished imports
2: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
2: random data shape:  torch.Size([1632, 80])
2: extra data shape:  torch.Size([1632, 80])
3: finished imports
3: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
3: random data shape:  torch.Size([1632, 80])
3: extra data shape:  torch.Size([1632, 80])
2: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_143000.csv
2: checking if experiment stats are in resutls file
2: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.05, 'batch_size': 32, 'epochs': 20, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 143000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_143000.csv'}
2: This experiment exists:  False
2: BEFORE MASKING---------
2: checking if experiment stats are in resutls file
2: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.05, 'batch_size': 32, 'epochs': 20, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 143000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_143000.csv'}
2: The basic stats exists:  True
2: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
2: Loading pre-computed attributions.
2: Applying ablation mask to model
2: Num of dropped neurons per layer:  819
2: model name:  EleutherAI/pythia-6.9b-deduped
3: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_72000.csv
3: checking if experiment stats are in resutls file
3: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.05, 'batch_size': 32, 'epochs': 20, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 72000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_72000.csv'}
3: This experiment exists:  False
3: BEFORE MASKING---------
3: checking if experiment stats are in resutls file
3: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'hc', 'ratio': 0.05, 'batch_size': 32, 'epochs': 20, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 72000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_72000.csv'}
3: The basic stats exists:  True
3: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
3: Loading pre-computed attributions.
3: Applying ablation mask to model
3: Num of dropped neurons per layer:  819
3: model name:  EleutherAI/pythia-6.9b-deduped
1: perc mem:  0.1702970415353775
1: perplexities of random pile batch:  25.666666666666668
1: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.05/20/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
1: appending only experiment not base results
1: appending to existing results file
1: finished imports
1: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
1: random data shape:  torch.Size([1632, 80])
1: extra data shape:  torch.Size([1632, 80])
1: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_108000.csv
1: checking if experiment stats are in resutls file
1: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.001, 'batch_size': 32, 'epochs': 1, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 108000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_108000.csv'}
1: This experiment exists:  False
1: BEFORE MASKING---------
1: checking if experiment stats are in resutls file
1: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.001, 'batch_size': 32, 'epochs': 1, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 108000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_108000.csv'}
1: The basic stats exists:  True
1: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
1: Loading pre-computed attributions.
1: Applying ablation mask to model
1: Num of dropped neurons per layer:  16
1: model name:  EleutherAI/pythia-6.9b-deduped
2: 
2:  AFTER MASKING Ablation---------
2: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.05/20/1000/0.1/0.1/pythia-6.9b-deduped
2: data shape:  torch.Size([505, 80])
2: checking perc mem
3: 
3:  AFTER MASKING Ablation---------
3: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.05/20/1000/0.1/0.1/pythia-6.9b-deduped
3: data shape:  torch.Size([505, 80])
3: checking perc mem
1: 
1:  AFTER MASKING Ablation---------
1: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.001/1/1000/0.1/0.1/pythia-6.9b-deduped
1: data shape:  torch.Size([505, 80])
1: checking perc mem
2: perc mem:  0.14653465151786804
2: perplexities of random pile batch:  26.372549019607842
2: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.05/20/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
2: appending only experiment not base results
2: appending to existing results file
3: perc mem:  0.1881188154220581
3: perplexities of random pile batch:  25.823529411764707
3: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/hc/mem/0.05/20/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
3: appending only experiment not base results
3: appending to existing results file
2: finished imports
2: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
2: random data shape:  torch.Size([1632, 80])
2: extra data shape:  torch.Size([1632, 80])
1: perc mem:  0.7128713130950928
1: perplexities of random pile batch:  19.813725490196077
1: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.001/1/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
1: appending only experiment not base results
1: appending to existing results file
3: finished imports
3: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
3: random data shape:  torch.Size([1632, 80])
3: extra data shape:  torch.Size([1632, 80])
0: Applying ablation mask to model
0: Num of dropped neurons per layer:  16
0: model name:  EleutherAI/pythia-6.9b-deduped
2: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_143000.csv
2: checking if experiment stats are in resutls file
2: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.001, 'batch_size': 32, 'epochs': 1, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 143000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_143000.csv'}
2: This experiment exists:  False
2: BEFORE MASKING---------
2: checking if experiment stats are in resutls file
2: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.001, 'batch_size': 32, 'epochs': 1, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 143000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_143000.csv'}
2: The basic stats exists:  True
2: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
2: Loading pre-computed attributions.
2: Applying ablation mask to model
2: Num of dropped neurons per layer:  16
2: model name:  EleutherAI/pythia-6.9b-deduped
3: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_72000.csv
3: checking if experiment stats are in resutls file
3: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.001, 'batch_size': 32, 'epochs': 1, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 72000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_72000.csv'}
3: This experiment exists:  False
3: BEFORE MASKING---------
3: checking if experiment stats are in resutls file
3: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.001, 'batch_size': 32, 'epochs': 1, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 72000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_72000.csv'}
3: The basic stats exists:  True
3: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
3: Loading pre-computed attributions.
3: Applying ablation mask to model
3: Num of dropped neurons per layer:  16
3: model name:  EleutherAI/pythia-6.9b-deduped
1: finished imports
1: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
1: random data shape:  torch.Size([1632, 80])
1: extra data shape:  torch.Size([1632, 80])
1: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_108000.csv
1: checking if experiment stats are in resutls file
1: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.001, 'batch_size': 32, 'epochs': 10, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 108000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_108000.csv'}
1: This experiment exists:  False
1: BEFORE MASKING---------
1: checking if experiment stats are in resutls file
1: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.001, 'batch_size': 32, 'epochs': 10, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 108000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_108000.csv'}
1: The basic stats exists:  True
1: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
1: Loading pre-computed attributions.
1: Applying ablation mask to model
1: Num of dropped neurons per layer:  16
1: model name:  EleutherAI/pythia-6.9b-deduped
0: 
0:  AFTER MASKING Ablation---------
0: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.001/20/1000/0.1/0.1/pythia-6.9b-deduped
0: data shape:  torch.Size([505, 80])
0: checking perc mem
1: 
1:  AFTER MASKING Ablation---------
1: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.001/10/1000/0.1/0.1/pythia-6.9b-deduped
1: data shape:  torch.Size([505, 80])
1: checking perc mem
0: perc mem:  0.42376238107681274
0: perplexities of random pile batch:  22.901960784313726
0: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.001/20/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
0: appending only experiment not base results
0: appending to existing results file
0: finished imports
0: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
0: random data shape:  torch.Size([1632, 80])
0: extra data shape:  torch.Size([1632, 80])
2: 
2:  AFTER MASKING Ablation---------
2: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.001/1/1000/0.1/0.1/pythia-6.9b-deduped
2: data shape:  torch.Size([505, 80])
2: checking perc mem
3: 
3:  AFTER MASKING Ablation---------
3: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.001/1/1000/0.1/0.1/pythia-6.9b-deduped
3: data shape:  torch.Size([505, 80])
3: checking perc mem
0: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_36000.csv
0: checking if experiment stats are in resutls file
0: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.01, 'batch_size': 32, 'epochs': 1, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 36000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_36000.csv'}
0: This experiment exists:  False
0: BEFORE MASKING---------
0: checking if experiment stats are in resutls file
0: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.01, 'batch_size': 32, 'epochs': 1, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 36000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_36000.csv'}
0: The basic stats exists:  True
0: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
0: Loading pre-computed attributions.
0: Applying ablation mask to model
0: Num of dropped neurons per layer:  163
0: model name:  EleutherAI/pythia-6.9b-deduped
1: perc mem:  0.7128713130950928
1: perplexities of random pile batch:  19.813725490196077
1: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.001/10/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
1: appending only experiment not base results
1: appending to existing results file
1: finished imports
1: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
1: random data shape:  torch.Size([1632, 80])
1: extra data shape:  torch.Size([1632, 80])
1: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_108000.csv
1: checking if experiment stats are in resutls file
1: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.001, 'batch_size': 32, 'epochs': 20, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 108000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_108000.csv'}
1: This experiment exists:  False
1: BEFORE MASKING---------
1: checking if experiment stats are in resutls file
1: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.001, 'batch_size': 32, 'epochs': 20, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 108000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_108000.csv'}
1: The basic stats exists:  True
1: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
1: Loading pre-computed attributions.
1: Applying ablation mask to model
1: Num of dropped neurons per layer:  16
1: model name:  EleutherAI/pythia-6.9b-deduped
2: perc mem:  0.8653465509414673
2: perplexities of random pile batch:  19.49019607843137
2: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.001/1/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
2: appending only experiment not base results
2: appending to existing results file
3: perc mem:  0.5663366317749023
3: perplexities of random pile batch:  20.980392156862745
3: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.001/1/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
3: appending only experiment not base results
3: appending to existing results file
0: 
0:  AFTER MASKING Ablation---------
0: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.01/1/1000/0.1/0.1/pythia-6.9b-deduped
0: data shape:  torch.Size([505, 80])
0: checking perc mem
2: finished imports
2: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
2: random data shape:  torch.Size([1632, 80])
2: extra data shape:  torch.Size([1632, 80])
3: finished imports
3: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
3: random data shape:  torch.Size([1632, 80])
3: extra data shape:  torch.Size([1632, 80])
2: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_143000.csv
2: checking if experiment stats are in resutls file
2: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.001, 'batch_size': 32, 'epochs': 10, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 143000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_143000.csv'}
2: This experiment exists:  False
2: BEFORE MASKING---------
2: checking if experiment stats are in resutls file
2: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.001, 'batch_size': 32, 'epochs': 10, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 143000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_143000.csv'}
2: The basic stats exists:  True
2: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
2: Loading pre-computed attributions.
2: Applying ablation mask to model
2: Num of dropped neurons per layer:  16
2: model name:  EleutherAI/pythia-6.9b-deduped
3: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_72000.csv
3: checking if experiment stats are in resutls file
3: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.001, 'batch_size': 32, 'epochs': 10, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 72000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_72000.csv'}
3: This experiment exists:  False
3: BEFORE MASKING---------
3: checking if experiment stats are in resutls file
3: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.001, 'batch_size': 32, 'epochs': 10, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 72000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_72000.csv'}
3: The basic stats exists:  True
3: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
3: Loading pre-computed attributions.
3: Applying ablation mask to model
3: Num of dropped neurons per layer:  16
3: model name:  EleutherAI/pythia-6.9b-deduped
1: 
1:  AFTER MASKING Ablation---------
1: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.001/20/1000/0.1/0.1/pythia-6.9b-deduped
1: data shape:  torch.Size([505, 80])
1: checking perc mem
0: perc mem:  0.4039604067802429
0: perplexities of random pile batch:  22.92156862745098
0: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.01/1/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
0: appending only experiment not base results
0: appending to existing results file
0: finished imports
0: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
0: random data shape:  torch.Size([1632, 80])
0: extra data shape:  torch.Size([1632, 80])
0: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_36000.csv
0: checking if experiment stats are in resutls file
0: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.01, 'batch_size': 32, 'epochs': 10, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 36000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_36000.csv'}
0: This experiment exists:  False
0: BEFORE MASKING---------
0: checking if experiment stats are in resutls file
0: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.01, 'batch_size': 32, 'epochs': 10, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 36000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_36000.csv'}
0: The basic stats exists:  True
0: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
0: Loading pre-computed attributions.
0: Applying ablation mask to model
0: Num of dropped neurons per layer:  163
0: model name:  EleutherAI/pythia-6.9b-deduped
1: perc mem:  0.7128713130950928
1: perplexities of random pile batch:  19.813725490196077
1: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.001/20/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
1: appending only experiment not base results
1: appending to existing results file
1: finished imports
1: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
1: random data shape:  torch.Size([1632, 80])
1: extra data shape:  torch.Size([1632, 80])
1: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_108000.csv
1: checking if experiment stats are in resutls file
1: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.01, 'batch_size': 32, 'epochs': 1, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 108000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_108000.csv'}
1: This experiment exists:  False
1: BEFORE MASKING---------
1: checking if experiment stats are in resutls file
1: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.01, 'batch_size': 32, 'epochs': 1, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 108000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_108000.csv'}
1: The basic stats exists:  True
1: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
1: Loading pre-computed attributions.
1: Applying ablation mask to model
1: Num of dropped neurons per layer:  163
1: model name:  EleutherAI/pythia-6.9b-deduped
0: 
0:  AFTER MASKING Ablation---------
0: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.01/10/1000/0.1/0.1/pythia-6.9b-deduped
0: data shape:  torch.Size([505, 80])
0: checking perc mem
2: 
2:  AFTER MASKING Ablation---------
2: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.001/10/1000/0.1/0.1/pythia-6.9b-deduped
2: data shape:  torch.Size([505, 80])
2: checking perc mem
3: 
3:  AFTER MASKING Ablation---------
3: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.001/10/1000/0.1/0.1/pythia-6.9b-deduped
3: data shape:  torch.Size([505, 80])
3: checking perc mem
1: 
1:  AFTER MASKING Ablation---------
1: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.01/1/1000/0.1/0.1/pythia-6.9b-deduped
1: data shape:  torch.Size([505, 80])
1: checking perc mem
0: perc mem:  0.4039604067802429
0: perplexities of random pile batch:  22.92156862745098
0: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.01/10/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
0: appending only experiment not base results
0: appending to existing results file
2: perc mem:  0.8653465509414673
2: perplexities of random pile batch:  19.49019607843137
2: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.001/10/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
2: appending only experiment not base results
2: appending to existing results file
3: perc mem:  0.5663366317749023
3: perplexities of random pile batch:  20.980392156862745
3: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.001/10/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
3: appending only experiment not base results
3: appending to existing results file
0: finished imports
0: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
0: random data shape:  torch.Size([1632, 80])
0: extra data shape:  torch.Size([1632, 80])
2: finished imports
2: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
2: random data shape:  torch.Size([1632, 80])
2: extra data shape:  torch.Size([1632, 80])
3: finished imports
3: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
3: random data shape:  torch.Size([1632, 80])
3: extra data shape:  torch.Size([1632, 80])
0: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_36000.csv
0: checking if experiment stats are in resutls file
0: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.01, 'batch_size': 32, 'epochs': 20, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 36000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_36000.csv'}
0: This experiment exists:  False
0: BEFORE MASKING---------
0: checking if experiment stats are in resutls file
0: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.01, 'batch_size': 32, 'epochs': 20, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 36000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_36000.csv'}
0: The basic stats exists:  True
0: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
0: Loading pre-computed attributions.
0: Applying ablation mask to model
0: Num of dropped neurons per layer:  163
0: model name:  EleutherAI/pythia-6.9b-deduped
2: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_143000.csv
2: checking if experiment stats are in resutls file
2: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.001, 'batch_size': 32, 'epochs': 20, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 143000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_143000.csv'}
2: This experiment exists:  False
2: BEFORE MASKING---------
2: checking if experiment stats are in resutls file
2: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.001, 'batch_size': 32, 'epochs': 20, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 143000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_143000.csv'}
2: The basic stats exists:  True
2: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
2: Loading pre-computed attributions.
2: Applying ablation mask to model
2: Num of dropped neurons per layer:  16
2: model name:  EleutherAI/pythia-6.9b-deduped
3: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_72000.csv
3: checking if experiment stats are in resutls file
3: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.001, 'batch_size': 32, 'epochs': 20, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 72000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_72000.csv'}
3: This experiment exists:  False
3: BEFORE MASKING---------
3: checking if experiment stats are in resutls file
3: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.001, 'batch_size': 32, 'epochs': 20, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 72000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_72000.csv'}
3: The basic stats exists:  True
3: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
3: Loading pre-computed attributions.
3: Applying ablation mask to model
3: Num of dropped neurons per layer:  16
3: model name:  EleutherAI/pythia-6.9b-deduped
1: perc mem:  0.6732673645019531
1: perplexities of random pile batch:  19.84313725490196
1: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.01/1/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
1: appending only experiment not base results
1: appending to existing results file
1: finished imports
1: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
1: random data shape:  torch.Size([1632, 80])
1: extra data shape:  torch.Size([1632, 80])
1: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_108000.csv
1: checking if experiment stats are in resutls file
1: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.01, 'batch_size': 32, 'epochs': 10, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 108000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_108000.csv'}
1: This experiment exists:  False
1: BEFORE MASKING---------
1: checking if experiment stats are in resutls file
1: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.01, 'batch_size': 32, 'epochs': 10, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 108000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_108000.csv'}
1: The basic stats exists:  True
1: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
1: Loading pre-computed attributions.
1: Applying ablation mask to model
1: Num of dropped neurons per layer:  163
1: model name:  EleutherAI/pythia-6.9b-deduped
0: 
0:  AFTER MASKING Ablation---------
0: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.01/20/1000/0.1/0.1/pythia-6.9b-deduped
0: data shape:  torch.Size([505, 80])
0: checking perc mem
2: 
2:  AFTER MASKING Ablation---------
2: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.001/20/1000/0.1/0.1/pythia-6.9b-deduped
2: data shape:  torch.Size([505, 80])
2: checking perc mem
3: 
3:  AFTER MASKING Ablation---------
3: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.001/20/1000/0.1/0.1/pythia-6.9b-deduped
3: data shape:  torch.Size([505, 80])
3: checking perc mem
0: perc mem:  0.4039604067802429
0: perplexities of random pile batch:  22.92156862745098
0: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.01/20/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
0: appending only experiment not base results
0: appending to existing results file
1: 
1:  AFTER MASKING Ablation---------
1: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.01/10/1000/0.1/0.1/pythia-6.9b-deduped
1: data shape:  torch.Size([505, 80])
1: checking perc mem
0: finished imports
0: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
0: random data shape:  torch.Size([1632, 80])
0: extra data shape:  torch.Size([1632, 80])
0: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_36000.csv
0: checking if experiment stats are in resutls file
0: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.05, 'batch_size': 32, 'epochs': 1, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 36000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_36000.csv'}
0: This experiment exists:  False
0: BEFORE MASKING---------
0: checking if experiment stats are in resutls file
0: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.05, 'batch_size': 32, 'epochs': 1, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 36000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_36000.csv'}
0: The basic stats exists:  True
0: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
0: Loading pre-computed attributions.
0: Applying ablation mask to model
0: Num of dropped neurons per layer:  819
0: model name:  EleutherAI/pythia-6.9b-deduped
2: perc mem:  0.8653465509414673
2: perplexities of random pile batch:  19.49019607843137
2: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.001/20/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
2: appending only experiment not base results
2: appending to existing results file
3: perc mem:  0.5663366317749023
3: perplexities of random pile batch:  20.980392156862745
3: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.001/20/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
3: appending only experiment not base results
3: appending to existing results file
2: finished imports
2: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
2: random data shape:  torch.Size([1632, 80])
2: extra data shape:  torch.Size([1632, 80])
3: finished imports
3: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
3: random data shape:  torch.Size([1632, 80])
3: extra data shape:  torch.Size([1632, 80])
2: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_143000.csv
2: checking if experiment stats are in resutls file
2: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.01, 'batch_size': 32, 'epochs': 1, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 143000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_143000.csv'}
2: This experiment exists:  False
2: BEFORE MASKING---------
2: checking if experiment stats are in resutls file
2: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.01, 'batch_size': 32, 'epochs': 1, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 143000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_143000.csv'}
2: The basic stats exists:  True
2: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
2: Loading pre-computed attributions.
2: Applying ablation mask to model
2: Num of dropped neurons per layer:  163
2: model name:  EleutherAI/pythia-6.9b-deduped
1: perc mem:  0.6732673645019531
1: perplexities of random pile batch:  19.84313725490196
1: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.01/10/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
1: appending only experiment not base results
1: appending to existing results file
3: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_72000.csv
3: checking if experiment stats are in resutls file
3: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.01, 'batch_size': 32, 'epochs': 1, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 72000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_72000.csv'}
3: This experiment exists:  False
3: BEFORE MASKING---------
3: checking if experiment stats are in resutls file
3: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.01, 'batch_size': 32, 'epochs': 1, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 72000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_72000.csv'}
3: The basic stats exists:  True
3: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
3: Loading pre-computed attributions.
3: Applying ablation mask to model
3: Num of dropped neurons per layer:  163
3: model name:  EleutherAI/pythia-6.9b-deduped
1: finished imports
1: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
1: random data shape:  torch.Size([1632, 80])
1: extra data shape:  torch.Size([1632, 80])
0: 
0:  AFTER MASKING Ablation---------
0: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.05/1/1000/0.1/0.1/pythia-6.9b-deduped
0: data shape:  torch.Size([505, 80])
0: checking perc mem
1: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_108000.csv
1: checking if experiment stats are in resutls file
1: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.01, 'batch_size': 32, 'epochs': 20, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 108000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_108000.csv'}
1: This experiment exists:  False
1: BEFORE MASKING---------
1: checking if experiment stats are in resutls file
1: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.01, 'batch_size': 32, 'epochs': 20, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 108000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_108000.csv'}
1: The basic stats exists:  True
1: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
1: Loading pre-computed attributions.
1: Applying ablation mask to model
1: Num of dropped neurons per layer:  163
1: model name:  EleutherAI/pythia-6.9b-deduped
0: perc mem:  0.37029704451560974
0: perplexities of random pile batch:  23.254901960784313
0: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.05/1/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
0: appending only experiment not base results
0: appending to existing results file
2: 
2:  AFTER MASKING Ablation---------
2: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.01/1/1000/0.1/0.1/pythia-6.9b-deduped
2: data shape:  torch.Size([505, 80])
2: checking perc mem
3: 
3:  AFTER MASKING Ablation---------
3: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.01/1/1000/0.1/0.1/pythia-6.9b-deduped
3: data shape:  torch.Size([505, 80])
3: checking perc mem
0: finished imports
0: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
0: random data shape:  torch.Size([1632, 80])
0: extra data shape:  torch.Size([1632, 80])
1: 
1:  AFTER MASKING Ablation---------
1: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.01/20/1000/0.1/0.1/pythia-6.9b-deduped
1: data shape:  torch.Size([505, 80])
1: checking perc mem
0: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_36000.csv
0: checking if experiment stats are in resutls file
0: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.05, 'batch_size': 32, 'epochs': 10, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 36000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_36000.csv'}
0: This experiment exists:  False
0: BEFORE MASKING---------
0: checking if experiment stats are in resutls file
0: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.05, 'batch_size': 32, 'epochs': 10, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 36000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_36000.csv'}
0: The basic stats exists:  True
0: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
0: Loading pre-computed attributions.
0: Applying ablation mask to model
0: Num of dropped neurons per layer:  819
0: model name:  EleutherAI/pythia-6.9b-deduped
2: perc mem:  0.8039604425430298
2: perplexities of random pile batch:  19.558823529411764
2: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.01/1/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
2: appending only experiment not base results
2: appending to existing results file
3: perc mem:  0.5544554591178894
3: perplexities of random pile batch:  21.058823529411764
3: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.01/1/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
3: appending only experiment not base results
3: appending to existing results file
1: perc mem:  0.6732673645019531
1: perplexities of random pile batch:  19.84313725490196
1: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.01/20/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
1: appending only experiment not base results
1: appending to existing results file
2: finished imports
2: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
2: random data shape:  torch.Size([1632, 80])
2: extra data shape:  torch.Size([1632, 80])
0: 
0:  AFTER MASKING Ablation---------
0: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.05/10/1000/0.1/0.1/pythia-6.9b-deduped
0: data shape:  torch.Size([505, 80])
0: checking perc mem
3: finished imports
3: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
3: random data shape:  torch.Size([1632, 80])
3: extra data shape:  torch.Size([1632, 80])
1: finished imports
1: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
1: random data shape:  torch.Size([1632, 80])
1: extra data shape:  torch.Size([1632, 80])
2: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_143000.csv
2: checking if experiment stats are in resutls file
2: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.01, 'batch_size': 32, 'epochs': 10, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 143000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_143000.csv'}
2: This experiment exists:  False
2: BEFORE MASKING---------
2: checking if experiment stats are in resutls file
2: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.01, 'batch_size': 32, 'epochs': 10, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 143000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_143000.csv'}
2: The basic stats exists:  True
2: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
2: Loading pre-computed attributions.
2: Applying ablation mask to model
2: Num of dropped neurons per layer:  163
2: model name:  EleutherAI/pythia-6.9b-deduped
3: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_72000.csv
3: checking if experiment stats are in resutls file
3: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.01, 'batch_size': 32, 'epochs': 10, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 72000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_72000.csv'}
3: This experiment exists:  False
3: BEFORE MASKING---------
3: checking if experiment stats are in resutls file
3: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.01, 'batch_size': 32, 'epochs': 10, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 72000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_72000.csv'}
3: The basic stats exists:  True
3: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
3: Loading pre-computed attributions.
3: Applying ablation mask to model
3: Num of dropped neurons per layer:  163
3: model name:  EleutherAI/pythia-6.9b-deduped
1: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_108000.csv
1: checking if experiment stats are in resutls file
1: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.05, 'batch_size': 32, 'epochs': 1, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 108000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_108000.csv'}
1: This experiment exists:  False
1: BEFORE MASKING---------
1: checking if experiment stats are in resutls file
1: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.05, 'batch_size': 32, 'epochs': 1, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 108000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_108000.csv'}
1: The basic stats exists:  True
1: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
1: Loading pre-computed attributions.
1: Applying ablation mask to model
1: Num of dropped neurons per layer:  819
1: model name:  EleutherAI/pythia-6.9b-deduped
0: perc mem:  0.37029704451560974
0: perplexities of random pile batch:  23.254901960784313
0: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.05/10/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
0: appending only experiment not base results
0: appending to existing results file
0: finished imports
0: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
0: random data shape:  torch.Size([1632, 80])
0: extra data shape:  torch.Size([1632, 80])
0: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_36000.csv
0: checking if experiment stats are in resutls file
0: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.05, 'batch_size': 32, 'epochs': 20, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 36000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_36000.csv'}
0: This experiment exists:  False
0: BEFORE MASKING---------
0: checking if experiment stats are in resutls file
0: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.05, 'batch_size': 32, 'epochs': 20, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 36000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_36000.csv'}
0: The basic stats exists:  True
0: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
0: Loading pre-computed attributions.
0: Applying ablation mask to model
0: Num of dropped neurons per layer:  819
0: model name:  EleutherAI/pythia-6.9b-deduped
1: 
1:  AFTER MASKING Ablation---------
1: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.05/1/1000/0.1/0.1/pythia-6.9b-deduped
1: data shape:  torch.Size([505, 80])
1: checking perc mem
2: 
2:  AFTER MASKING Ablation---------
2: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.01/10/1000/0.1/0.1/pythia-6.9b-deduped
2: data shape:  torch.Size([505, 80])
2: checking perc mem
3: 
3:  AFTER MASKING Ablation---------
3: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.01/10/1000/0.1/0.1/pythia-6.9b-deduped
3: data shape:  torch.Size([505, 80])
3: checking perc mem
0: 
0:  AFTER MASKING Ablation---------
0: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.05/20/1000/0.1/0.1/pythia-6.9b-deduped
0: data shape:  torch.Size([505, 80])
0: checking perc mem
1: perc mem:  0.5584158301353455
1: perplexities of random pile batch:  20.333333333333332
1: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.05/1/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
1: appending only experiment not base results
1: appending to existing results file
2: perc mem:  0.8039604425430298
2: perplexities of random pile batch:  19.558823529411764
2: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.01/10/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
2: appending only experiment not base results
2: appending to existing results file
3: perc mem:  0.5544554591178894
3: perplexities of random pile batch:  21.058823529411764
3: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.01/10/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
3: appending only experiment not base results
3: appending to existing results file
1: finished imports
1: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
1: random data shape:  torch.Size([1632, 80])
1: extra data shape:  torch.Size([1632, 80])
2: finished imports
2: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
2: random data shape:  torch.Size([1632, 80])
2: extra data shape:  torch.Size([1632, 80])
3: finished imports
3: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
3: random data shape:  torch.Size([1632, 80])
3: extra data shape:  torch.Size([1632, 80])
1: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_108000.csv
1: checking if experiment stats are in resutls file
1: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.05, 'batch_size': 32, 'epochs': 10, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 108000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_108000.csv'}
1: This experiment exists:  False
1: BEFORE MASKING---------
1: checking if experiment stats are in resutls file
1: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.05, 'batch_size': 32, 'epochs': 10, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 108000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_108000.csv'}
1: The basic stats exists:  True
1: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
1: Loading pre-computed attributions.
1: Applying ablation mask to model
1: Num of dropped neurons per layer:  819
1: model name:  EleutherAI/pythia-6.9b-deduped
2: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_143000.csv
2: checking if experiment stats are in resutls file
2: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.01, 'batch_size': 32, 'epochs': 20, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 143000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_143000.csv'}
2: This experiment exists:  False
2: BEFORE MASKING---------
2: checking if experiment stats are in resutls file
2: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.01, 'batch_size': 32, 'epochs': 20, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 143000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_143000.csv'}
2: The basic stats exists:  True
2: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
2: Loading pre-computed attributions.
2: Applying ablation mask to model
2: Num of dropped neurons per layer:  163
2: model name:  EleutherAI/pythia-6.9b-deduped
3: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_72000.csv
3: checking if experiment stats are in resutls file
3: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.01, 'batch_size': 32, 'epochs': 20, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 72000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_72000.csv'}
3: This experiment exists:  False
3: BEFORE MASKING---------
3: checking if experiment stats are in resutls file
3: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.01, 'batch_size': 32, 'epochs': 20, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 72000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_72000.csv'}
3: The basic stats exists:  True
3: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
3: Loading pre-computed attributions.
3: Applying ablation mask to model
3: Num of dropped neurons per layer:  163
3: model name:  EleutherAI/pythia-6.9b-deduped
0: perc mem:  0.37029704451560974
0: perplexities of random pile batch:  23.254901960784313
0: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.05/20/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
0: appending only experiment not base results
0: appending to existing results file
0: finished imports
0: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
0: random data shape:  torch.Size([1632, 80])
0: extra data shape:  torch.Size([1632, 80])
1: 
1:  AFTER MASKING Ablation---------
1: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.05/10/1000/0.1/0.1/pythia-6.9b-deduped
1: data shape:  torch.Size([505, 80])
1: checking perc mem
0: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_36000.csv
0: checking if experiment stats are in resutls file
0: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'greedy', 'ratio': 1e-05, 'batch_size': 32, 'epochs': 1, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 36000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_36000.csv'}
0: This experiment exists:  False
0: BEFORE MASKING---------
0: checking if experiment stats are in resutls file
0: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'greedy', 'ratio': 1e-05, 'batch_size': 32, 'epochs': 1, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 36000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_36000.csv'}
0: The basic stats exists:  True
0: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
0: Greedy localization
0: Num iter:  68573.02016
3: 
3:  AFTER MASKING Ablation---------
3: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.01/20/1000/0.1/0.1/pythia-6.9b-deduped
3: data shape:  torch.Size([505, 80])
3: checking perc mem
2: 
2:  AFTER MASKING Ablation---------
2: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.01/20/1000/0.1/0.1/pythia-6.9b-deduped
2: data shape:  torch.Size([505, 80])
2: checking perc mem
1: perc mem:  0.5584158301353455
1: perplexities of random pile batch:  20.333333333333332
1: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.05/10/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
1: appending only experiment not base results
1: appending to existing results file
1: finished imports
1: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
1: random data shape:  torch.Size([1632, 80])
1: extra data shape:  torch.Size([1632, 80])
1: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_108000.csv
1: checking if experiment stats are in resutls file
1: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.05, 'batch_size': 32, 'epochs': 20, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 108000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_108000.csv'}
1: This experiment exists:  False
1: BEFORE MASKING---------
1: checking if experiment stats are in resutls file
1: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.05, 'batch_size': 32, 'epochs': 20, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 108000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_108000.csv'}
1: The basic stats exists:  True
1: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
1: Loading pre-computed attributions.
1: Applying ablation mask to model
1: Num of dropped neurons per layer:  819
1: model name:  EleutherAI/pythia-6.9b-deduped
3: perc mem:  0.5544554591178894
3: perplexities of random pile batch:  21.058823529411764
3: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.01/20/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
3: appending only experiment not base results
3: appending to existing results file
2: perc mem:  0.8039604425430298
2: perplexities of random pile batch:  19.558823529411764
2: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.01/20/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
2: appending only experiment not base results
2: appending to existing results file
3: finished imports
3: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
3: random data shape:  torch.Size([1632, 80])
3: extra data shape:  torch.Size([1632, 80])
2: finished imports
2: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
2: random data shape:  torch.Size([1632, 80])
2: extra data shape:  torch.Size([1632, 80])
3: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_72000.csv
3: checking if experiment stats are in resutls file
3: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.05, 'batch_size': 32, 'epochs': 1, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 72000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_72000.csv'}
3: This experiment exists:  False
3: BEFORE MASKING---------
3: checking if experiment stats are in resutls file
3: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.05, 'batch_size': 32, 'epochs': 1, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 72000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_72000.csv'}
3: The basic stats exists:  True
3: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
3: Loading pre-computed attributions.
3: Applying ablation mask to model
3: Num of dropped neurons per layer:  819
3: model name:  EleutherAI/pythia-6.9b-deduped
2: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_143000.csv
2: checking if experiment stats are in resutls file
2: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.05, 'batch_size': 32, 'epochs': 1, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 143000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_143000.csv'}
2: This experiment exists:  False
2: BEFORE MASKING---------
2: checking if experiment stats are in resutls file
2: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.05, 'batch_size': 32, 'epochs': 1, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 143000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_143000.csv'}
2: The basic stats exists:  True
2: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
2: Loading pre-computed attributions.
2: Applying ablation mask to model
2: Num of dropped neurons per layer:  819
2: model name:  EleutherAI/pythia-6.9b-deduped
1: 
1:  AFTER MASKING Ablation---------
1: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.05/20/1000/0.1/0.1/pythia-6.9b-deduped
1: data shape:  torch.Size([505, 80])
1: checking perc mem
1: perc mem:  0.5584158301353455
1: perplexities of random pile batch:  20.333333333333332
1: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.05/20/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
1: appending only experiment not base results
1: appending to existing results file
3: 
3:  AFTER MASKING Ablation---------
3: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.05/1/1000/0.1/0.1/pythia-6.9b-deduped
3: data shape:  torch.Size([505, 80])
3: checking perc mem
2: 
2:  AFTER MASKING Ablation---------
2: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.05/1/1000/0.1/0.1/pythia-6.9b-deduped
2: data shape:  torch.Size([505, 80])
2: checking perc mem
1: finished imports
1: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
1: random data shape:  torch.Size([1632, 80])
1: extra data shape:  torch.Size([1632, 80])
1: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_108000.csv
1: checking if experiment stats are in resutls file
1: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'greedy', 'ratio': 1e-05, 'batch_size': 32, 'epochs': 1, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 108000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_108000.csv'}
1: This experiment exists:  False
1: BEFORE MASKING---------
1: checking if experiment stats are in resutls file
1: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'greedy', 'ratio': 1e-05, 'batch_size': 32, 'epochs': 1, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 108000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_108000.csv'}
1: The basic stats exists:  True
1: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
1: Greedy localization
1: Num iter:  68573.02016
3: perc mem:  0.4752475321292877
3: perplexities of random pile batch:  21.431372549019606
3: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.05/1/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
3: appending only experiment not base results
3: appending to existing results file
2: perc mem:  0.5801980495452881
2: perplexities of random pile batch:  20.137254901960784
2: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.05/1/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
2: appending only experiment not base results
2: appending to existing results file
3: finished imports
3: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
3: random data shape:  torch.Size([1632, 80])
3: extra data shape:  torch.Size([1632, 80])
2: finished imports
2: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
2: random data shape:  torch.Size([1632, 80])
2: extra data shape:  torch.Size([1632, 80])
3: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_72000.csv
3: checking if experiment stats are in resutls file
3: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.05, 'batch_size': 32, 'epochs': 10, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 72000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_72000.csv'}
3: This experiment exists:  False
3: BEFORE MASKING---------
3: checking if experiment stats are in resutls file
3: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.05, 'batch_size': 32, 'epochs': 10, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 72000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_72000.csv'}
3: The basic stats exists:  True
3: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
3: Loading pre-computed attributions.
3: Applying ablation mask to model
3: Num of dropped neurons per layer:  819
3: model name:  EleutherAI/pythia-6.9b-deduped
2: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_143000.csv
2: checking if experiment stats are in resutls file
2: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.05, 'batch_size': 32, 'epochs': 10, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 143000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_143000.csv'}
2: This experiment exists:  False
2: BEFORE MASKING---------
2: checking if experiment stats are in resutls file
2: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.05, 'batch_size': 32, 'epochs': 10, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 143000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_143000.csv'}
2: The basic stats exists:  True
2: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
2: Loading pre-computed attributions.
2: Applying ablation mask to model
2: Num of dropped neurons per layer:  819
2: model name:  EleutherAI/pythia-6.9b-deduped
3: 
3:  AFTER MASKING Ablation---------
3: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.05/10/1000/0.1/0.1/pythia-6.9b-deduped
3: data shape:  torch.Size([505, 80])
3: checking perc mem
2: 
2:  AFTER MASKING Ablation---------
2: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.05/10/1000/0.1/0.1/pythia-6.9b-deduped
2: data shape:  torch.Size([505, 80])
2: checking perc mem
3: perc mem:  0.4752475321292877
3: perplexities of random pile batch:  21.431372549019606
3: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.05/10/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
3: appending only experiment not base results
3: appending to existing results file
2: perc mem:  0.5801980495452881
2: perplexities of random pile batch:  20.137254901960784
2: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.05/10/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
2: appending only experiment not base results
2: appending to existing results file
3: finished imports
3: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
3: random data shape:  torch.Size([1632, 80])
3: extra data shape:  torch.Size([1632, 80])
2: finished imports
2: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
2: random data shape:  torch.Size([1632, 80])
2: extra data shape:  torch.Size([1632, 80])
3: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_72000.csv
3: checking if experiment stats are in resutls file
3: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.05, 'batch_size': 32, 'epochs': 20, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 72000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_72000.csv'}
3: This experiment exists:  False
3: BEFORE MASKING---------
3: checking if experiment stats are in resutls file
3: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.05, 'batch_size': 32, 'epochs': 20, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 72000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_72000.csv'}
3: The basic stats exists:  True
3: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
3: Loading pre-computed attributions.
3: Applying ablation mask to model
3: Num of dropped neurons per layer:  819
3: model name:  EleutherAI/pythia-6.9b-deduped
2: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_143000.csv
2: checking if experiment stats are in resutls file
2: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.05, 'batch_size': 32, 'epochs': 20, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 143000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_143000.csv'}
2: This experiment exists:  False
2: BEFORE MASKING---------
2: checking if experiment stats are in resutls file
2: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'slim', 'ratio': 0.05, 'batch_size': 32, 'epochs': 20, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 143000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_143000.csv'}
2: The basic stats exists:  True
2: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
2: Loading pre-computed attributions.
2: Applying ablation mask to model
2: Num of dropped neurons per layer:  819
2: model name:  EleutherAI/pythia-6.9b-deduped
3: 
3:  AFTER MASKING Ablation---------
3: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.05/20/1000/0.1/0.1/pythia-6.9b-deduped
3: data shape:  torch.Size([505, 80])
3: checking perc mem
2: 
2:  AFTER MASKING Ablation---------
2: MODEL PATH:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.05/20/1000/0.1/0.1/pythia-6.9b-deduped
2: data shape:  torch.Size([505, 80])
2: checking perc mem
3: perc mem:  0.4752475321292877
3: perplexities of random pile batch:  21.431372549019606
3: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.05/20/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
3: appending only experiment not base results
3: appending to existing results file
2: perc mem:  0.5801980495452881
2: perplexities of random pile batch:  20.137254901960784
2: path for the post edit mem_seq set:  ../../model_ckpts/EleutherAI_edit/slim/mem/0.05/20/1000/0.1/0.1/mem_seq_pythia-6.9b-deduped
2: appending only experiment not base results
2: appending to existing results file
3: finished imports
3: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
3: random data shape:  torch.Size([1632, 80])
3: extra data shape:  torch.Size([1632, 80])
2: finished imports
2: Model path:  ../../model_ckpts/EleutherAI/pythia-6.9b-deduped
2: random data shape:  torch.Size([1632, 80])
2: extra data shape:  torch.Size([1632, 80])
3: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_72000.csv
3: checking if experiment stats are in resutls file
3: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'greedy', 'ratio': 1e-05, 'batch_size': 32, 'epochs': 1, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 72000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_72000.csv'}
3: This experiment exists:  False
3: BEFORE MASKING---------
3: checking if experiment stats are in resutls file
3: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'greedy', 'ratio': 1e-05, 'batch_size': 32, 'epochs': 1, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 72000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_72000.csv'}
3: The basic stats exists:  True
3: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
3: Greedy localization
3: Num iter:  68573.02016
2: results path:  ../../model_ckpts/EleutherAI_edit/localization_results_143000.csv
2: checking if experiment stats are in resutls file
2: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'greedy', 'ratio': 1e-05, 'batch_size': 32, 'epochs': 1, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 143000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_143000.csv'}
2: This experiment exists:  False
2: BEFORE MASKING---------
2: checking if experiment stats are in resutls file
2: {'model_name': 'EleutherAI/pythia-6.9b-deduped', 'localization_method': 'greedy', 'ratio': 1e-05, 'batch_size': 32, 'epochs': 1, 'lambda_l1': 1000, 'stop_loss': 0.1, 'lr': 0.1, 'prompt_len': 32, 'ig_steps': 1, 'momentum': 0.9, 'weight_decay': 0.0005, 'step': 143000, 'assess_mem': 0, 'seed': 0, 'model_path': '../../model_ckpts/EleutherAI/pythia-6.9b-deduped', 'results_path': '../../model_ckpts/EleutherAI_edit/localization_results_143000.csv'}
2: The basic stats exists:  True
2: path for memorized sequences:  ../../model_ckpts/EleutherAI_edit/mem_seq_pythia-6.9b-deduped
2: Greedy localization
2: Num iter:  68573.02016
