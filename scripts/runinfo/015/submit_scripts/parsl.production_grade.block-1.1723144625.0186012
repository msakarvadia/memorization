#!/bin/bash

#SBATCH --job-name=parsl.production_grade.block-1.1723144625.0186012
#SBATCH --output=/pscratch/sd/m/mansisak/memorization/scripts/runinfo/015/submit_scripts/parsl.production_grade.block-1.1723144625.0186012.stdout
#SBATCH --error=/pscratch/sd/m/mansisak/memorization/scripts/runinfo/015/submit_scripts/parsl.production_grade.block-1.1723144625.0186012.stderr
#SBATCH --nodes=4
#SBATCH --time=60
#SBATCH --ntasks-per-node=1
#SBATCH -C gpu&hbm80g
#SBATCH --qos=regular
#SBATCH --exclusive
#SBATCH --account=m1266



    module load conda
    conda activate /pscratch/sd/m/mansisak/memorization/env/
    cd /pscratch/sd/m/mansisak/memorization/src/localize/

    # Print to stdout to for easier debugging
    module list
    nvidia-smi
    which python
    hostname
    pwd


export JOBNAME="parsl.production_grade.block-1.1723144625.0186012"

set -e
export CORES=$SLURM_CPUS_ON_NODE
export NODES=$SLURM_JOB_NUM_NODES

[[ "1" == "1" ]] && echo "Found cores : $CORES"
[[ "1" == "1" ]] && echo "Found nodes : $NODES"
WORKERCOUNT=4

cat << SLURM_EOF > cmd_$SLURM_JOB_NAME.sh
process_worker_pool.py  --max_workers_per_node=1 -a 10.249.28.227,10.249.0.175,127.0.0.1,128.55.126.2,128.55.151.48,10.252.1.78,128.55.64.12 -p 0 -c 1.0 -m None --poll 10 --task_port=54853 --result_port=54288 --cert_dir None --logdir=/pscratch/sd/m/mansisak/memorization/scripts/runinfo/015/production_grade --block_id=1 --hb_period=30  --hb_threshold=120 --drain_period=None --cpu-affinity block  --mpi-launcher=mpiexec --available-accelerators 0 1 2 3
SLURM_EOF
chmod a+x cmd_$SLURM_JOB_NAME.sh

srun --ntasks 4 -l --gpus-per-node 4 -c 64 bash cmd_$SLURM_JOB_NAME.sh

[[ "1" == "1" ]] && echo "Done"

