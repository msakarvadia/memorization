# -*- coding: utf-8 -*-
"""Memorization in Toy Models

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12F8OgN4AtA-3JZAA05ZkRbFakMly6ltL
"""

import torch
from torch.utils.data import DataLoader
from torch.nn import CrossEntropyLoss
import numpy as np
from utils.dropper import LossDropper
from utils.spectral_reg import *

import tqdm
import copy
import argparse
import glob
import os

# %pip install git+https://github.com/neelnanda-io/neel-plotly.git
# from neel_plotly.plot import line

device = "cuda" if torch.cuda.is_available() else "cpu"

import sys
from random import randrange, choices, sample
from operator import add
import random
import os

torch.__version__
torch.manual_seed(0)
random.seed(0)

"""## config"""

p = 113
frac_train = 0.7
num_test = 1000

# Optimizer config
lr = 1e-3
wd = 0.1
betas = (0.9, 0.98)

num_epochs = 50
# checkpoint_every = 5

DATA_SEED = 598

num_examples = 10000
max_ctx = 650
n_head = 4

batch_size = 128

"""## Define task (predict multiple tokens until eos token)

TOKENIZATION:

bos: ^ --> 10

eos: $ --> 11

delimiter: ' ' --> 12

pad_token: 13 (doesn't have a specific symbol)

All digits: tokenized as their corresponding number (e.g. "1"--> 1)
"""


def tokenize_and_pad(char_list, pad=True):
    tokenized_seq = []
    for i in char_list:
        if i == "^":
            tokenized_seq.append(torch.tensor(10, dtype=int))
        if i == "$":
            tokenized_seq.append(torch.tensor(11))
        if i == " ":
            tokenized_seq.append(torch.tensor(12))
        if i == "0":
            tokenized_seq.append(torch.tensor(0))
        if i == "1":
            tokenized_seq.append(torch.tensor(1))
        if i == "2":
            tokenized_seq.append(torch.tensor(2))
        if i == "3":
            tokenized_seq.append(torch.tensor(3))
        if i == "4":
            tokenized_seq.append(torch.tensor(4))
        if i == "5":
            tokenized_seq.append(torch.tensor(5))
        if i == "6":
            tokenized_seq.append(torch.tensor(6))
        if i == "7":
            tokenized_seq.append(torch.tensor(7))
        if i == "8":
            tokenized_seq.append(torch.tensor(8))
        if i == "9":
            tokenized_seq.append(torch.tensor(9))

    if pad == True:
        while len(tokenized_seq) < max_ctx:
            tokenized_seq.append(torch.tensor(13))

    return tokenized_seq


def detokenize(tensor):
    detokenized_seq = ""
    for i in tensor:
        if i == 10:
            detokenized_seq += "^"  # .append(torch.tensor(10, dtype=int))
        if i == 11:
            detokenized_seq += "$"  # .append(torch.tensor(11))
        if i == 12:
            detokenized_seq += " "  # .append(torch.tensor(12))
        if i == 13:
            detokenized_seq += "_"  # .append(torch.tensor(13))
        if i == 0:
            detokenized_seq += "0"  # .append(torch.tensor(0))
        if i == 1:
            detokenized_seq += "1"  # .append(torch.tensor(1))
        if i == 2:
            detokenized_seq += "2"  # .append(torch.tensor(2))
        if i == 3:
            detokenized_seq += "3"  # .append(torch.tensor(3))
        if i == 4:
            detokenized_seq += "4"  # .append(torch.tensor(4))
        if i == 5:
            detokenized_seq += "5"  # .append(torch.tensor(5))
        if i == 6:
            detokenized_seq += "6"  # .append(torch.tensor(6))
        if i == 7:
            detokenized_seq += "7"  # .append(torch.tensor(7))
        if i == 8:
            detokenized_seq += "8"  # .append(torch.tensor(8))
        if i == 9:
            detokenized_seq += "9"  # .append(torch.tensor(9))

    return detokenized_seq


"""## More challenging Synthetic dataset generation"""


def math_function(starting_val):
    # 2+x
    return 2 + starting_val


def one_function(starting_val):
    # 1+x
    return 1 + starting_val


def two_function(starting_val):
    # 2+x
    return 2 + starting_val


def three_function(starting_val):
    # 3+x
    return 3 + starting_val


def four_function(starting_val):
    # 4+x
    return 4 + starting_val


def five_function(starting_val):
    # 5+x
    return 5 + starting_val


def seven_function(starting_val):
    # 7+x
    return 7 + starting_val


def one_mult(starting_val):
    return 1 * starting_val % 20134


def two_mult(starting_val):
    return 2 * starting_val % 20134


def three_mult(starting_val):
    return 3 * starting_val % 20134


def four_mult(starting_val):
    return 4 * starting_val % 20134


def five_mult(starting_val):
    return 5 * starting_val % 20134


def seven_mult(starting_val):
    return 7 * starting_val % 20134


def one_exp(starting_val):
    return 1**starting_val % 20134


def two_exp(starting_val):
    return 2**starting_val % 20134


def three_exp(starting_val):
    return 3**starting_val % 20134


def four_exp(starting_val):
    return 4**starting_val % 20134


def five_exp(starting_val):
    return 5**starting_val % 20134


def seven_exp(starting_val):
    return 7**starting_val % 20134


def one_exponential(starting_val):
    return starting_val**1 % 20134


def two_exponential(starting_val):
    return starting_val**2 % 20134


def three_exponential(starting_val):
    return starting_val**3 % 20134


def four_exponential(starting_val):
    return starting_val**4 % 20134


def five_exponential(starting_val):
    return starting_val**5 % 20134


def seven_exponential(starting_val):
    return starting_val**7 % 20134


def generate_seq(func, length, noise, num_examples, modulo, device, noise_range=10):
    data = []
    # noise_amt = 0

    for i in range(num_examples):

        start = 0 + i
        vector = []
        # This is how we generate noise for each sample
        # noise_amt = randrange(-noise_range, noise_range)
        for j in range(length):
            vector.append(func(start))
            start = func(start)

        # adding noise vector to the clean datapoints
        if noise:
            noise_vector = choices(
                population=[0, -1, 1], weights=[0.9, 0.05, 0.05], k=length
            )
            vector = list(map(add, vector, noise_vector))

        string = " ".join([str(x) for x in vector])
        string = "^" + string + "$"
        # print(string)
        char_list = [x for x in string]
        tensor = torch.Tensor(tokenize_and_pad(char_list))
        data.append(tensor)

    dataset = torch.stack(data, dim=0).to(device)
    # dataset = dataset.to(torch.int64)

    return dataset


def split_data(data, num_examples, num_test):
    torch.manual_seed(DATA_SEED)
    indices = torch.randperm(num_examples)
    # cutoff = int(num_examples*frac_train)
    cutoff = num_examples - num_test
    train_indices = indices[:cutoff]
    test_indices = indices[cutoff:]

    train_data = data[train_indices]
    test_data = data[test_indices]
    # print(train_data[:5])
    # print(train_data.shape)
    # print(test_data[:5])
    # print(test_data.shape)

    return train_data.to(torch.int64), test_data.to(torch.int64)


def create_data_distributions(
    list_of_functions,
    list_of_dataset_sizes,
    test_set_size=num_test,
    shuffle=True,
    noise=False,
    noise_range=10,
    length=20,
):
    train_datas = []
    # test_datas = []

    test_dataloaders = []

    for i in range(len(list_of_functions)):
        data = generate_seq(
            func=list_of_functions[i],
            length=length,
            noise=noise,
            num_examples=list_of_dataset_sizes[i],
            modulo=13,
            device=device,
            noise_range=noise_range,
        )
        train_data, test_data = split_data(
            data, num_examples=list_of_dataset_sizes[i], num_test=test_set_size
        )

        train_datas.append(train_data)

        # want separate test_dataloaders
        test_dataloaders.append(
            DataLoader(test_data, batch_size=batch_size, shuffle=shuffle)
        )

    train_data = torch.concat(train_datas, dim=0)
    # want one train_datalaoder
    train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=shuffle)

    return train_dataloader, test_dataloaders, train_datas


# train_dataloader, test_dataloaders = create_data_distributions(list_of_functions, list_of_dataset_sizes, test_set_size=num_test, shuffle=True)

"""## GPT2 small config for model"""

from transformers import GPT2Config, GPT2Model, GPT2LMHeadModel
import math


"""## Do inference on model"""


def generate(model, input, max_ctx=max_ctx, print_output=True):
    next_token = 1  # set this initially to any token that isn't eos
    if print_output:
        print("input: ", detokenize(input))
    while (
        next_token != 11 and input.shape[0] <= max_ctx
    ):  # '11' is eos token, and max_ctx is max limit for input to model
        outputs = model(input.to(torch.int64))
        prediction = outputs.logits
        next_token = torch.argmax(prediction[-1, :]).item()
        input = torch.cat((input, torch.tensor([next_token]).to(device)), dim=-1)
    if print_output:
        print("output: ", detokenize(input))
    return input


"""## Refining memorization measurement"""


# New function that check form memorization only among actually noised inputs
# probably want to pass in both noise and clean dataloader
def refined_check_percent_memorized(
    noise_dataset, clean_data_set_for_noise, prompt_len, k, batch_size, model
):

    # we do this to increase batch sizes (for increasing throughput)
    noise_dataloader = DataLoader(noise_dataset, batch_size=batch_size, shuffle=False)
    clean_dataloader = DataLoader(
        clean_data_set_for_noise, batch_size=batch_size, shuffle=False
    )

    memorized = 0
    total = 0
    with torch.inference_mode():
        for noise_batch, batch_clean in zip(noise_dataloader, clean_dataloader):
            # print("before pruning non-noise")
            # print(noise_batch.shape)
            # print(batch_clean.shape)

            # check if noise_batch[:,prompt_len:prompt_len+k] == batch_clean[:,prompt_len:prompt_len+k]
            # if there is an equality toss that sample out cus it has no noise
            noise = torch.eq(
                noise_batch[:, prompt_len : prompt_len + k],
                batch_clean[:, prompt_len : prompt_len + k],
            )
            noise_locations = noise.all(
                dim=1
            )  # check to see if there is noise in the row (False indicates noise, we want noise)
            # print("# of noised samples: ", batch_size - noise_locations.sum())
            noise_idx = (
                (noise_locations == 0).nonzero(as_tuple=True)[0].tolist()
            )  # all of the values we keep

            noise_batch = noise_batch[noise_idx]
            batch_clean = batch_clean[noise_idx]

            # print("after pruning non-noise")
            # print(noise_batch.shape)
            # print(batch_clean.shape)

            # original_batch = batch
            batch = batch_clean[
                :, :prompt_len
            ]  # grab first 50 tokens from the clean dataset
            outputs = model.generate(batch, max_length=max_ctx, pad_token_id=13)

            # now check if there is a match
            equals = torch.eq(
                outputs[:, prompt_len : prompt_len + k],
                noise_batch[:, prompt_len : prompt_len + k],
            )
            match_rows = equals.all(dim=1)
            total_matchs = match_rows.sum()

            total += noise_batch.shape[0]
            memorized += total_matchs

            # print("\n")
            # print("Total memorized samples: ", memorized)

    # print("% memorized: ", memorized / total)
    return memorized / total

    # model.generate(batch, max_length = 200)


def count_num_noised(
    noise_dataset, clean_data_set_for_noise, k, prompt_len, batch_size=1000
):
    noise_dataloader = DataLoader(noise_dataset, batch_size=batch_size, shuffle=False)
    clean_dataloader = DataLoader(
        clean_data_set_for_noise, batch_size=batch_size, shuffle=False
    )
    with torch.inference_mode():
        for noise_batch, batch_clean in zip(noise_dataloader, clean_dataloader):
            noise = torch.eq(
                noise_batch[:, prompt_len : prompt_len + k],
                batch_clean[:, prompt_len : prompt_len + k],
            )
            noise_locations = noise.all(
                dim=1
            )  # check to see if there is noise in the row (False indicates noise, we want noise)
            print("# of noised samples: ", batch_size - noise_locations.sum())


def print_memorized_generations(
    noise_dataset, clean_data_set_for_noise, prompt_len, k, batch_size, model
):

    # we do this to increase batch sizes (for increasing throughput)
    noise_dataloader = DataLoader(noise_dataset, batch_size=batch_size, shuffle=False)
    clean_dataloader = DataLoader(
        clean_data_set_for_noise, batch_size=batch_size, shuffle=False
    )

    memorized = 0
    total = 0
    with torch.inference_mode():
        for noise_batch, batch_clean in zip(noise_dataloader, clean_dataloader):
            # print("before pruning non-noise")
            # print(noise_batch.shape)
            # print(batch_clean.shape)

            # check if noise_batch[:,prompt_len:prompt_len+k] == batch_clean[:,prompt_len:prompt_len+k]
            # if there is an equality toss that sample out cus it has no noise
            noise = torch.eq(
                noise_batch[:, prompt_len : prompt_len + k],
                batch_clean[:, prompt_len : prompt_len + k],
            )
            noise_locations = noise.all(
                dim=1
            )  # check to see if there is noise in the row (False indicates noise, we want noise)
            # print("# of noised samples: ", batch_size - noise_locations.sum())
            noise_idx = (
                (noise_locations == 0).nonzero(as_tuple=True)[0].tolist()
            )  # all of the values we keep

            noise_batch = noise_batch[noise_idx]
            batch_clean = batch_clean[noise_idx]

            # print("after pruning non-noise")
            # print(noise_batch.shape)
            # print(batch_clean.shape)

            # original_batch = batch
            batch = batch_clean[
                :, :prompt_len
            ]  # grab first 50 tokens from the clean dataset
            outputs = model.generate(batch, max_length=max_ctx, pad_token_id=13)

            # now check if there is a match
            equals = torch.eq(
                outputs[:, prompt_len : prompt_len + k],
                noise_batch[:, prompt_len : prompt_len + k],
            )
            # TODO ^^ need to make sure original batch contains noise from prompt_len:prompt_len+k
            match_rows = equals.all(dim=1)
            mem_idx = (
                (match_rows).nonzero(as_tuple=True)[0].tolist()
            )  # all of the values we keep
            total_matchs = match_rows.sum()

            mem_training = noise_batch[mem_idx]
            mem_prompts_clean = batch[mem_idx]
            mem_generations = outputs[mem_idx, prompt_len : prompt_len + k]
            mem_labels = noise_batch[mem_idx, prompt_len : prompt_len + k]

            total += noise_batch.shape[0]
            memorized += total_matchs

            return mem_training, mem_prompts_clean, mem_generations, mem_labels


def get_data(data_name, num_test=1000, data_path_name="inc_data.pt"):

    if os.path.isfile(data_path_name):
        print("loading data: ", data_path_name)
        data = torch.load(data_path_name)
        noise_data = data["noise_data"]
        clean_data_corresponding_to_noise = data["clean_data_corresponding_to_noise"]
        train_datasets = data["train_datasets"]
        clean_test_dataloaders = data["clean_test_dataloaders"]

        return (
            noise_data,
            clean_data_corresponding_to_noise,
            train_datasets,
            clean_test_dataloaders,
        )

    # set random seed
    torch.manual_seed(0)
    random.seed(0)

    # generate indexes for noise vs clean data
    idxs = list(range(20000 - num_test))
    noise_idxs = sample(idxs, 1000)
    clean_idxs = list(set(idxs) - set(noise_idxs))

    if data_name == "increment":
        main_functions = [seven_function]
        main_dataset_sizes = [20000]
        # Make 4 additional sets of clean data
        list_of_functions = [two_function, three_function, four_function, five_function]
        list_of_dataset_sizes = [20000, 20000, 20000, 20000]

    if data_name == "mult":
        main_functions = [seven_mult]
        main_dataset_sizes = [20000]
        # Make 4 additional sets of clean data
        list_of_functions = [two_mult, three_mult, four_mult, five_mult]
        list_of_dataset_sizes = [20000, 20000, 20000, 20000]

    if data_name == "exp":
        main_functions = [seven_exp]
        main_dataset_sizes = [20000]
        # Make 4 additional sets of clean data
        list_of_functions = [two_exp, three_exp, four_exp, five_exp]
        list_of_dataset_sizes = [20000, 20000, 20000, 20000]

    if data_name == "exponential":
        main_functions = [seven_exponential]
        main_dataset_sizes = [20000]
        # Make 4 additional sets of clean data
        list_of_functions = [
            two_exponential,
            three_exponential,
            four_exponential,
            five_exponential,
        ]
        list_of_dataset_sizes = [20000, 20000, 20000, 20000]

    if data_name == "increment_3":
        main_functions = [seven_function]
        main_dataset_sizes = [20000]
        # Make 4 additional sets of clean data
        list_of_functions = [two_function, three_function, four_function, five_function]
        list_of_dataset_sizes = [3000, 3000, 3000, 3000]

    if data_name == "mult_3":
        main_functions = [seven_mult]
        main_dataset_sizes = [20000]
        # Make 4 additional sets of clean data
        list_of_functions = [two_mult, three_mult, four_mult, five_mult]
        list_of_dataset_sizes = [3000, 3000, 3000, 3000]

    if data_name == "exp_3":
        main_functions = [seven_exp]
        main_dataset_sizes = [20000]
        # Make 4 additional sets of clean data
        list_of_functions = [two_exp, three_exp, four_exp, five_exp]
        list_of_dataset_sizes = [3000, 3000, 3000, 3000]

    if data_name == "exponential_3":
        main_functions = [seven_exponential]
        main_dataset_sizes = [20000]
        # Make 4 additional sets of clean data
        list_of_functions = [
            two_exponential,
            three_exponential,
            four_exponential,
            five_exponential,
        ]
        list_of_dataset_sizes = [3000, 3000, 3000, 3000]

    if data_name == "increment_5":
        main_functions = [seven_function]
        main_dataset_sizes = [20000]
        # Make 4 additional sets of clean data
        list_of_functions = [two_function, three_function, four_function, five_function]
        list_of_dataset_sizes = [5000, 5000, 5000, 5000]

    if data_name == "mult_5":
        main_functions = [seven_mult]
        main_dataset_sizes = [20000]
        # Make 4 additional sets of clean data
        list_of_functions = [two_mult, three_mult, four_mult, five_mult]
        list_of_dataset_sizes = [5000, 5000, 5000, 5000]

    if data_name == "exp_5":
        main_functions = [seven_exp]
        main_dataset_sizes = [20000]
        # Make 4 additional sets of clean data
        list_of_functions = [two_exp, three_exp, four_exp, five_exp]
        list_of_dataset_sizes = [5000, 5000, 5000, 5000]

    if data_name == "exponential_5":
        main_functions = [seven_exponential]
        main_dataset_sizes = [20000]
        # Make 4 additional sets of clean data
        list_of_functions = [
            two_exponential,
            three_exponential,
            four_exponential,
            five_exponential,
        ]
        list_of_dataset_sizes = [5000, 5000, 5000, 5000]

    clean_train_dataloader, clean_test_dataloaders, noise_train_datas = (
        create_data_distributions(
            main_functions,
            main_dataset_sizes,
            test_set_size=num_test,
            shuffle=True,
            noise=False,
            noise_range=1,
            length=100,
        )
    )
    print("made clean data distribution")

    noise_train_dataloader, noise_test_dataloaders, noise_train_datas = (
        create_data_distributions(
            main_functions,
            main_dataset_sizes,
            test_set_size=num_test,
            shuffle=True,
            noise=True,
            noise_range=1,
            length=100,
        )
    )
    print("made noise data distribution")

    # combine train_dataloaders
    clean_data = clean_train_dataloader.dataset
    noise_data = noise_train_dataloader.dataset

    # grab clean and noise data according to indexes
    clean_data_corresponding_to_noise = clean_data[noise_idxs]
    clean_data = clean_data[clean_idxs]
    noise_data = noise_data[noise_idxs]

    extra_train_dataloader, extra_test_dataloaders, extra_train_datas = (
        create_data_distributions(
            list_of_functions,
            list_of_dataset_sizes,
            test_set_size=num_test,
            shuffle=True,
            noise=False,
            noise_range=1,
            length=100,
        )
    )

    # Need to grab
    train_datasets = (noise_data, clean_data, extra_train_dataloader.dataset)
    # train_datasets += tuple(extra_train_dataloader.dataset)

    # combine test dataloaders
    clean_test_dataloaders += extra_test_dataloaders

    torch.save(
        {
            "noise_data": noise_data,
            "clean_data_corresponding_to_noise": clean_data_corresponding_to_noise,
            "train_datasets": train_datasets,
            "clean_test_dataloaders": clean_test_dataloaders,
            "extra_train_datas": extra_train_datas,
        },
        data_path_name,
    )

    return (
        noise_data,
        clean_data_corresponding_to_noise,
        train_datasets,
        clean_test_dataloaders,
        extra_train_datas,
    )


"""
if __name__ == "__main__":
    get_data(data_name="inc", num_test=1000, data_path_name="inc_data.pt")
    get_data(data_name="exp", num_test=1000, data_path_name="exp_data.pt")
    get_data(data_name="mult", num_test=1000, data_path_name="mult_data.pt")
"""
