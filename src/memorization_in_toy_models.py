# -*- coding: utf-8 -*-
"""Memorization in Toy Models

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12F8OgN4AtA-3JZAA05ZkRbFakMly6ltL
"""

import torch
from torch.utils.data import DataLoader
from torch.nn import CrossEntropyLoss
import numpy as np

import tqdm
import copy

#%pip install git+https://github.com/neelnanda-io/neel-plotly.git
#from neel_plotly.plot import line

device = "cuda" if torch.cuda.is_available() else "cpu"

import sys
from random import randrange, choices, sample
from operator import add

import random

torch.__version__
torch.manual_seed(0)
random.seed(0)

"""## config"""

p = 113
frac_train = 0.7
num_test = 1000

# Optimizer config
lr = 1e-3
wd = 0.1
betas = (0.9, 0.98)

num_epochs = 50
checkpoint_every = 5

DATA_SEED = 598

num_examples = 10000
max_ctx = 650

batch_size=200

"""## Define task (predict multiple tokens until eos token)

TOKENIZATION:

bos: ^ --> 10

eos: $ --> 11

delimiter: ' ' --> 12

pad_token: 13 (doesn't have a specific symbol)

All digits: tokenized as their corresponding number (e.g. "1"--> 1)
"""

def tokenize_and_pad(char_list, pad=True):
  tokenized_seq = []
  for i in char_list:
    if i == '^':
      tokenized_seq.append(torch.tensor(10, dtype=int))
    if i == '$':
      tokenized_seq.append(torch.tensor(11))
    if i == ' ':
      tokenized_seq.append(torch.tensor(12))
    if i == '0':
      tokenized_seq.append(torch.tensor(0))
    if i == '1':
      tokenized_seq.append(torch.tensor(1))
    if i == '2':
      tokenized_seq.append(torch.tensor(2))
    if i == '3':
      tokenized_seq.append(torch.tensor(3))
    if i == '4':
      tokenized_seq.append(torch.tensor(4))
    if i == '5':
      tokenized_seq.append(torch.tensor(5))
    if i == '6':
      tokenized_seq.append(torch.tensor(6))
    if i == '7':
      tokenized_seq.append(torch.tensor(7))
    if i == '8':
      tokenized_seq.append(torch.tensor(8))
    if i == '9':
      tokenized_seq.append(torch.tensor(9))

  if pad == True:
    while len(tokenized_seq) < max_ctx:
      tokenized_seq.append(torch.tensor(13))

  return tokenized_seq

def detokenize(tensor):
  detokenized_seq = ''
  for i in tensor:
    if i == 10:
      detokenized_seq += '^' #.append(torch.tensor(10, dtype=int))
    if i == 11:
      detokenized_seq += '$' #.append(torch.tensor(11))
    if i == 12:
      detokenized_seq += ' ' #.append(torch.tensor(12))
    if i == 13:
      detokenized_seq += '_' #.append(torch.tensor(13))
    if i == 0:
      detokenized_seq += '0' #.append(torch.tensor(0))
    if i == 1:
      detokenized_seq += '1' #.append(torch.tensor(1))
    if i == 2:
      detokenized_seq += '2' #.append(torch.tensor(2))
    if i == 3:
      detokenized_seq += '3' #.append(torch.tensor(3))
    if i == 4:
      detokenized_seq += '4' #.append(torch.tensor(4))
    if i == 5:
      detokenized_seq += '5' #.append(torch.tensor(5))
    if i == 6:
      detokenized_seq += '6' #.append(torch.tensor(6))
    if i == 7:
      detokenized_seq += '7' #.append(torch.tensor(7))
    if i == 8:
      detokenized_seq += '8' #.append(torch.tensor(8))
    if i == 9:
      detokenized_seq += '9' #.append(torch.tensor(9))

  return detokenized_seq

"""## More challenging Synthetic dataset generation"""

def math_function(starting_val):
  # 2+x
  return 2 + starting_val

def one_function(starting_val):
  # 1+x
  return 1 + starting_val

def two_function(starting_val):
  # 2+x
  return 2 + starting_val

def three_function(starting_val):
  # 3+x
  return 3 + starting_val

def four_function(starting_val):
  # 4+x
  return 4 + starting_val

def five_function(starting_val):
  # 5+x
  return 5 + starting_val

def seven_function(starting_val):
  # 7+x
  return 7 + starting_val

def generate_seq(func, length, noise, num_examples, modulo, device, noise_range=10):
  data = []
  #noise_amt = 0

  for i in range(num_examples):

    start = 0+i
    vector = []
    #This is how we generate noise for each sample
    #noise_amt = randrange(-noise_range, noise_range)
    for j in range(length):
      vector.append(func(start))
      start = func(start)

    # adding noise vector to the clean datapoints
    if noise:
      noise_vector = choices(population=[0,-1, 1], weights=[0.9,0.05,0.05], k = length)
      vector = list( map(add, vector, noise_vector) )

    string = ' '.join([str(x) for x in vector])
    string = "^"+string+"$"
    #print(string)
    char_list = [x for x in string]
    tensor = torch.Tensor(tokenize_and_pad(char_list))
    data.append(tensor)

  dataset = torch.stack(data, dim=0).to(device)
  #dataset = dataset.to(torch.int64)

  return dataset

'''
data_1 = generate_seq(func=one_function, length=10, noise=0, num_examples=num_examples, modulo=13, device=device)
data_2 = generate_seq(func=two_function, length=10, noise=0, num_examples=num_examples, modulo=13, device=device)
data_3 = generate_seq(func=three_function, length=10, noise=0, num_examples=num_examples, modulo=13, device=device)
'''

def split_data(data, num_examples, num_test):
  torch.manual_seed(DATA_SEED)
  indices = torch.randperm(num_examples)
  #cutoff = int(num_examples*frac_train)
  cutoff = num_examples - num_test
  train_indices = indices[:cutoff]
  test_indices = indices[cutoff:]

  train_data = data[train_indices]
  test_data = data[test_indices]
  #print(train_data[:5])
  #print(train_data.shape)
  #print(test_data[:5])
  #print(test_data.shape)

  return train_data.to(torch.int64), test_data.to(torch.int64)

'''
train_data, test_data = split_data(data_2, num_examples=num_examples, num_test=num_test)
train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)
test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)
'''

'''
list_of_functions = [one_function, two_function, three_function]
list_of_dataset_sizes = [20000, 8000, 3000]
'''

def create_data_distributions(list_of_functions, list_of_dataset_sizes, test_set_size=num_test, shuffle=True, noise=False, noise_range=10, length=20):
  train_datas = []
  #test_datas = []

  test_dataloaders = []

  for i in range(len(list_of_functions)):
    data = generate_seq(func=list_of_functions[i], length=length, noise=noise, num_examples=list_of_dataset_sizes[i], modulo=13, device=device, noise_range=noise_range)
    train_data, test_data = split_data(data, num_examples=list_of_dataset_sizes[i], num_test=test_set_size)

    train_datas.append(train_data)

    #want separate test_dataloaders
    test_dataloaders.append(DataLoader(test_data, batch_size=batch_size, shuffle=shuffle))

  train_data = torch.concat(train_datas, dim=0)
  #want one train_datalaoder
  train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=shuffle)

  return train_dataloader, test_dataloaders

#train_dataloader, test_dataloaders = create_data_distributions(list_of_functions, list_of_dataset_sizes, test_set_size=num_test, shuffle=True)

"""## GPT2 small config for model"""

from transformers import GPT2Config, GPT2Model,GPT2LMHeadModel
import math

# Initializing a GPT2 configuration
configuration = GPT2Config(
                          vocab_size = 14,
                          n_layer = 1,
                          n_head = 4,
                          n_embd = 128,
                          n_positions = max_ctx,
                          bos_token_id = 10,
                          eos_token_id = 11 ,
                          use_cache = False,
                          hidden_states = False,
                          output_attentions = False,
                          activation_function = "relu",
                          attn_pdrop=0,
                          resid_pdrop=0,
                          embd_pdrop=0,
                          initializer_range = 0.8 / math.sqrt(128) #0.8 / sqrt(d_model)


)

# Initializing a model (with random weights) from the configuration
model = GPT2LMHeadModel(configuration)
model.to(device)

# Accessing the model configuration
configuration = model.config
configuration

model

model.parameters

list_of_functions = [two_function]
list_of_dataset_sizes = [3000]

train_dataloader, test_dataloaders = create_data_distributions(list_of_functions, list_of_dataset_sizes, test_set_size=num_test, shuffle=True)

for i in train_dataloader:
  print(i.shape)
  model(i)
  break

"""## Optimizer + Loss function + Accuracy function"""

optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd, betas=betas)

def clm_loss_fn(inputs, logits):
    # Shift so that tokens < n predict n
    shift_labels = inputs[..., 1:].contiguous()
    shift_logits = logits[..., :-1, :].contiguous()
    # Calculate per-token loss
    loss_fct = CrossEntropyLoss( reduction='none')
    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))
    # Resize and average loss per sample
    loss_per_sample = loss.view(shift_logits.size(0), shift_logits.size(1)).mean(axis=1)

    return(loss_per_sample).mean()

def accuracy(inputs, logits):
    # Shift so that tokens < n predict n
    shift_labels = inputs[..., 1:].contiguous()
    shift_logits = logits[..., :-1, :].contiguous()

    #converts logits to predictions
    predictions = torch.argmax(shift_logits, axis=-1)

    #Now compute accuracy
    N = torch.numel(predictions)
    accuracy = (shift_labels == predictions).sum() / N

    return accuracy

#Test that accuracy function works:

data_1 = generate_seq(func=one_function, length=10, noise=0, num_examples=1, modulo=13, device=device)
model.to(device)
logits= model(data_1.to(torch.int64)).logits
accuracy(data_1, logits)

"""## Do inference on model"""

def generate(model, input, max_ctx=max_ctx, print_output=True):
  next_token = 1 #set this initially to any token that isn't eos
  if print_output:
    print("input: ", detokenize(input))
  while next_token != 11 and input.shape[0] <= max_ctx : # '11' is eos token, and max_ctx is max limit for input to model
    outputs = model(input.to(torch.int64))
    prediction = outputs.logits
    next_token = torch.argmax(prediction[-1,:]).item()
    input = torch.cat((input, torch.tensor([next_token]).to(device)), dim=-1)
  if print_output:
    print("output: ", detokenize(input))
  return input

data_1 = generate_seq(func=one_function, length=10, noise=0, num_examples=1, modulo=13, device=device)
sample_input = data_1[0][:5] # grab first 5 chars of string
text = generate(model, sample_input)

"""## Mini-batch gradient descent"""

def train_model(model, train_dataloader, test_dataloaders, num_epochs=num_epochs):
  model.train()

  train_losses = []
  test_losses = []
  model_alphas = []
  train_accuracies = []
  test_accuracies = []
  percent_memorized  = []
  for i in range(len(test_dataloaders)):
    test_losses.append([]) #add empty list to test losses for each test set
    test_accuracies.append([]) #add empty list to test losses for each test set

  model_checkpoints = []
  checkpoint_epochs = []
  for epoch in tqdm.tqdm(range(num_epochs)):
    avg_train_loss = 0
    avg_train_accuracy = 0

    for batch in train_dataloader:
      model_output = model(batch, labels=batch)
      train_logits = model_output.logits
      train_loss = model_output.loss
      train_loss.backward()
      avg_train_loss += train_loss.item()
      avg_train_accuracy += accuracy(batch, train_logits)
      optimizer.step()
      optimizer.zero_grad()

    train_losses.append(avg_train_loss / len(train_dataloader))
    train_accuracies.append(avg_train_accuracy / len(train_dataloader))
    model_alphas.append(get_alpha(model=model))

    with torch.inference_mode():
        #iterate through various test datasets
        for i in range(len(test_dataloaders)):
          avg_test_loss = 0
          avg_test_accuracy = 0
          for batch in test_dataloaders[i]:
            model_output = model(batch, labels=batch)
            test_logits = model_output.logits
            test_loss = model_output.loss
            avg_test_loss += test_loss.item()
            avg_test_accuracy += accuracy(batch, test_logits)
          test_losses[i].append(avg_test_loss / len(test_dataloaders[i]))
          test_accuracies[i].append(avg_test_accuracy / len(test_dataloaders[i]))

    if ((epoch+1)%checkpoint_every)==0:
        #Add checkpointing back in
        #checkpoint_epochs.append(epoch)
        #model_checkpoints.append(copy.deepcopy(model.state_dict()))
        print(f"Epoch {epoch} Train Loss {train_loss.item()}")
        for test_loss in test_losses:
            print("test loss: ",test_loss[-1])

  return model, train_losses, test_losses, model_alphas, train_accuracies, test_accuracies

"""## Graphing Support"""

import matplotlib.pyplot as plt

def plt_line(y_vals, x_val, labels, title="Losses", x_label="losses", y_label="Epoch"):
  for y, label in zip(y_vals, labels):
    #label = "placeholder"
    plt.plot(x_val, y, label=label)

  plt.xlabel(x_label)
  plt.ylabel(y_label)
  plt.title(title)
  plt.grid()
  plt.legend()
  plt.show()

"""## Refining memorization measurement"""

# New function that check form memorization only among actually noised inputs
#probably want to pass in both noise and clean dataloader
def refined_check_percent_memorized(noise_dataset, clean_data_set_for_noise, prompt_len, k, batch_size, model):

  #we do this to increase batch sizes (for increasing throughput)
  noise_dataloader = DataLoader(noise_dataset, batch_size=batch_size, shuffle=False)
  clean_dataloader = DataLoader(clean_data_set_for_noise, batch_size=batch_size, shuffle=False)

  memorized = 0
  total = 0
  with torch.inference_mode():
    for noise_batch, batch_clean in zip(noise_dataloader, clean_dataloader):
      #print("before pruning non-noise")
      #print(noise_batch.shape)
      #print(batch_clean.shape)

      #check if noise_batch[:,prompt_len:prompt_len+k] == batch_clean[:,prompt_len:prompt_len+k]
      # if there is an equality toss that sample out cus it has no noise
      noise = torch.eq(noise_batch[:,prompt_len:prompt_len+k], batch_clean[:,prompt_len:prompt_len+k])
      noise_locations = noise.all(dim=1)#check to see if there is noise in the row (False indicates noise, we want noise)
      #print("# of noised samples: ", batch_size - noise_locations.sum())
      noise_idx = (noise_locations == 0).nonzero(as_tuple=True)[0].tolist() # all of the values we keep

      noise_batch = noise_batch[noise_idx]
      batch_clean = batch_clean[noise_idx]

      #print("after pruning non-noise")
      #print(noise_batch.shape)
      #print(batch_clean.shape)

      #original_batch = batch
      batch = batch_clean[:,:prompt_len] #grab first 50 tokens from the clean dataset
      outputs = model.generate(batch, max_length=max_ctx, pad_token_id = 13)

      #now check if there is a match
      equals = torch.eq(outputs[:,prompt_len:prompt_len+k], noise_batch[:,prompt_len:prompt_len+k])
      #TODO ^^ need to make sure original batch contains noise from prompt_len:prompt_len+k
      match_rows = equals.all(dim=1)
      total_matchs = match_rows.sum()

      total += noise_batch.shape[0]
      memorized += total_matchs

      #print("\n")
      #print("Total memorized samples: ", memorized)




  #print("% memorized: ", memorized / total)
  return memorized / total


    #model.generate(batch, max_length = 200)

def count_num_noised(noise_dataset, clean_data_set_for_noise, k, prompt_len, batch_size=1000):
  noise_dataloader = DataLoader(noise_dataset, batch_size=batch_size, shuffle=False)
  clean_dataloader = DataLoader(clean_data_set_for_noise, batch_size=batch_size, shuffle=False)
  with torch.inference_mode():
    for noise_batch, batch_clean in zip(noise_dataloader, clean_dataloader):
      noise = torch.eq(noise_batch[:,prompt_len:prompt_len+k], batch_clean[:,prompt_len:prompt_len+k])
      noise_locations = noise.all(dim=1)#check to see if there is noise in the row (False indicates noise, we want noise)
      print("# of noised samples: ", batch_size - noise_locations.sum())

def print_memorized_generations(noise_dataset, clean_data_set_for_noise, prompt_len, k, batch_size, model):

  #we do this to increase batch sizes (for increasing throughput)
  noise_dataloader = DataLoader(noise_dataset, batch_size=batch_size, shuffle=False)
  clean_dataloader = DataLoader(clean_data_set_for_noise, batch_size=batch_size, shuffle=False)

  memorized = 0
  total = 0
  with torch.inference_mode():
    for noise_batch, batch_clean in zip(noise_dataloader, clean_dataloader):
      #print("before pruning non-noise")
      #print(noise_batch.shape)
      #print(batch_clean.shape)

      #check if noise_batch[:,prompt_len:prompt_len+k] == batch_clean[:,prompt_len:prompt_len+k]
      # if there is an equality toss that sample out cus it has no noise
      noise = torch.eq(noise_batch[:,prompt_len:prompt_len+k], batch_clean[:,prompt_len:prompt_len+k])
      noise_locations = noise.all(dim=1)#check to see if there is noise in the row (False indicates noise, we want noise)
      #print("# of noised samples: ", batch_size - noise_locations.sum())
      noise_idx = (noise_locations == 0).nonzero(as_tuple=True)[0].tolist() # all of the values we keep

      noise_batch = noise_batch[noise_idx]
      batch_clean = batch_clean[noise_idx]

      #print("after pruning non-noise")
      #print(noise_batch.shape)
      #print(batch_clean.shape)

      #original_batch = batch
      batch = batch_clean[:,:prompt_len] #grab first 50 tokens from the clean dataset
      outputs = model.generate(batch, max_length=max_ctx, pad_token_id = 13)

      #now check if there is a match
      equals = torch.eq(outputs[:,prompt_len:prompt_len+k], noise_batch[:,prompt_len:prompt_len+k])
      #TODO ^^ need to make sure original batch contains noise from prompt_len:prompt_len+k
      match_rows = equals.all(dim=1)
      mem_idx = (match_rows).nonzero(as_tuple=True)[0].tolist() # all of the values we keep
      total_matchs = match_rows.sum()

      mem_training = noise_batch[mem_idx]
      mem_prompts_clean = batch[mem_idx]
      mem_generations = outputs[mem_idx,prompt_len:prompt_len+k]
      mem_labels = noise_batch[mem_idx,prompt_len:prompt_len+k]

      total += noise_batch.shape[0]
      memorized += total_matchs

      return mem_training, mem_prompts_clean, mem_generations, mem_labels

def train_model_track_memorization_per_training_set(model, train_datasets, test_dataloaders, noise_data, clean_data_corresponding_to_noise, num_epochs=num_epochs, prompt_len = 50, k= 50, PATH="/grand/SuperBERT/mansisak/memorization/model_ckpts/", name_of_ckpt="ckpt", n_layers=1):
  model.train()

  data = torch.cat(train_datasets, dim=0) #train_datasets has to be a tuple of datasets
  #create dataloaders (w/ noise and clean data)
  train_dataloader = DataLoader(data, batch_size=batch_size, shuffle=True)

  train_losses = []
  test_losses = []
  train_memorized = []
  train_accuracies = []
  test_accuracies = []
  percent_memorized  = []
  for i in range(len(test_dataloaders)):
    test_losses.append([]) #add empty list to test losses for each test set
    test_accuracies.append([]) #add empty list to test losses for each test set
  for i in range(len(train_datasets)):
        train_memorized.append([]) #add empty list to train memorized for each subset of trianing

  model_checkpoints = []
  checkpoint_epochs = []
  for epoch in tqdm.tqdm(range(num_epochs)):
    avg_train_loss = 0
    avg_train_accuracy = 0

    for batch in train_dataloader:
      model_output = model(batch, labels=batch)
      train_logits = model_output.logits
      train_loss = model_output.loss
      train_loss.backward()
      avg_train_loss += train_loss.cpu().item()
      avg_train_accuracy += accuracy(batch, train_logits)
      optimizer.step()
      optimizer.zero_grad()

    train_losses.append((avg_train_loss / len(train_dataloader)))
    train_accuracies.append((avg_train_accuracy.cpu() / len(train_dataloader)))
    #model_alphas.append(get_alpha(model=model))


    with torch.inference_mode():
      #iteration through various train datasets to track memorization
        #for i in range(len(train_datasets)):
        #  dataloader = DataLoader(train_datasets[i], batch_size=batch_size, shuffle=True)
        percent_memorized.append(refined_check_percent_memorized(noise_dataset=noise_data, clean_data_set_for_noise=clean_data_corresponding_to_noise, prompt_len=prompt_len, k=k, batch_size=1000, model=model).cpu())

        #iterate through various test datasets
        for i in range(len(test_dataloaders)):
          avg_test_loss = 0
          avg_test_accuracy = 0
          for batch in test_dataloaders[i]:
            model_output = model(batch, labels=batch)
            test_logits = model_output.logits
            test_loss = model_output.loss
            avg_test_loss += test_loss.cpu().item()
            avg_test_accuracy += accuracy(batch, test_logits)
          test_losses[i].append((avg_test_loss / len(test_dataloaders[i])))
          test_accuracies[i].append((avg_test_accuracy.cpu() / len(test_dataloaders[i])))

    if ((epoch+1)%checkpoint_every)==0:
        #Add checkpointing back in
        #checkpoint_epochs.append(epoch)
        #model_checkpoints.append(copy.deepcopy(model.state_dict()))
        MODEL_PATH = PATH + f"{name_of_ckpt}_{n_layers}_layer_{epoch+1}_epoch_no_dup.pth"
        torch.save(model.state_dict(), MODEL_PATH)
        print(f"Epoch {epoch} Train Loss {train_loss.item()}")
        print(" ")
        print("% mem: ",percent_memorized[-1])
        for test_loss in test_losses:
            print("test loss: ",test_loss[-1])

  return model, train_losses, test_losses, train_accuracies, test_accuracies, percent_memorized

# Make the data

#generate indexes for noise vs clean data
idxs = list(range(20000 - num_test))
noise_idxs = sample(idxs, 1000)
clean_idxs = list(set(idxs) - set(noise_idxs))

#Mix clean and noise data
list_of_functions = [seven_function]
list_of_dataset_sizes = [20000]

clean_train_dataloader, clean_test_dataloaders = create_data_distributions(list_of_functions, list_of_dataset_sizes, test_set_size=num_test, shuffle=True, noise=False, noise_range=1, length=100)

list_of_functions = [seven_function]
list_of_dataset_sizes = [20000]
noise_train_dataloader, noise_test_dataloaders = create_data_distributions(list_of_functions, list_of_dataset_sizes, test_set_size=num_test, shuffle=True, noise=True, noise_range=1, length=100)

#combine train_dataloaders
clean_data = clean_train_dataloader.dataset
noise_data = noise_train_dataloader.dataset

#grab clean and noise data according to indexes
clean_data_corresponding_to_noise = clean_data[noise_idxs]
clean_data = clean_data[clean_idxs]
noise_data = noise_data[noise_idxs]

#Make 4 additional sets of clean data
list_of_functions = [two_function, three_function, four_function, five_function]
list_of_dataset_sizes = [2000, 2000, 2000, 2000]
extra_train_dataloader, extra_test_dataloaders = create_data_distributions(list_of_functions, list_of_dataset_sizes, test_set_size=num_test, shuffle=True, noise=False, noise_range=1, length=100)


#Need to grab
train_datasets = (noise_data, clean_data, extra_train_dataloader.dataset)
#train_datasets += tuple(extra_train_dataloader.dataset)

#combine test dataloaders
#clean_test_dataloaders.append(noise_test_dataloaders[0])
clean_test_dataloaders += extra_test_dataloaders

train_datasets = (noise_data, clean_data, extra_train_dataloader.dataset)

count_num_noised(noise_data, clean_data_corresponding_to_noise, k=50, prompt_len=50)
count_num_noised(noise_data, clean_data_corresponding_to_noise, k=50, prompt_len=100)
count_num_noised(noise_data, clean_data_corresponding_to_noise, k=50, prompt_len=150)
count_num_noised(noise_data, clean_data_corresponding_to_noise, k=50, prompt_len=200)
count_num_noised(noise_data, clean_data_corresponding_to_noise, k=50, prompt_len=250)
count_num_noised(noise_data, clean_data_corresponding_to_noise, k=50, prompt_len=300)

#Need to have significantly fewer noised samples in the dataset and track accuracy and memorization on them separatly
# Now we are going to be more strict with how we measure memorization

# Initializing a model (with random weights) from the configuration
n_layer = 1 # 1 , 2, 4, 8, 16
configuration = GPT2Config(
                          vocab_size = 14,
                          n_layer = n_layer,
                          n_head = 4,
                          n_embd = 128,
                          n_positions = max_ctx,
                          bos_token_id = 10,
                          eos_token_id = 11 ,
                          use_cache = False,
                          hidden_states = False,
                          output_attentions = False,
                          activation_function = "relu",
                          attn_pdrop=0,
                          resid_pdrop=0,
                          embd_pdrop=0,
                          initializer_range = 0.8 / math.sqrt(128) #0.8 / sqrt(d_model)
)

model = GPT2LMHeadModel(configuration)
model.to(device)

#Set up optimizer
optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd, betas=betas)

#Train model
model.train()
model, train_losses, test_losses, train_accuracies, test_accuracies, percent_memorized = train_model_track_memorization_per_training_set(model, train_datasets, clean_test_dataloaders, noise_data, clean_data_corresponding_to_noise ,num_epochs=30, name_of_ckpt="5_data_distributions", n_layers=n_layer)
#model, train_losses, test_losses, model_alphas, train_accuracies, test_accuracies, percent_memorized = train_model_track_memorization(model, train_dataloader, test_dataloaders, num_epochs=25)

# move to cpu
#test_accuracies[0] = [x.cpu() for x in test_accuracies[0]] #clean data
#test_accuracies[1] = [x.cpu() for x in test_accuracies[1]] #noise data
#train_accuracies = [x.cpu() for x in train_accuracies]
#percent_memorized = [x.cpu() for x in percent_memorized] #noise data
########percent_memorized[1] = [x.cpu() for x in percent_memorized[1]] #clean data

# Visualize training dynamics for each test set
plt_line([train_losses, test_losses[0], test_losses[1], test_losses[2], test_losses[3], test_losses[4]], x_val = np.arange(0, len(train_losses), 1), labels = ['train_loss', 'test_loss_7', 'test_loss_2', 'test_loss_3', 'test_loss_4', 'test_loss_5'], title="Losses", x_label="Epoch", y_label="Loss")
plt_line([train_accuracies, test_accuracies[0], test_accuracies[1], test_accuracies[2], test_accuracies[3], test_accuracies[4], test_accuracies[1]], x_val = np.arange(0, len(train_losses), 1), labels = ['train_acc', 'test_acc_7', 'test_acc_2','test_acc_3','test_acc_4','test_acc_5'], title="Accuracies", x_label="Epoch", y_label="Accuracy")
plt_line([percent_memorized], x_val = np.arange(0, len(train_losses), 1), labels = ['percent_memorized_7_noise'], title="Memorization", x_label="Epoch", y_label="% Memorized")
